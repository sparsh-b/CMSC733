{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fMVszVsJSWCC"
      },
      "outputs": [],
      "source": [
        "## Import necessary libraries here (You can add libraries you want to use here)\n",
        "from skimage.util import img_as_float\n",
        "from skimage import io, color\n",
        "from skimage import draw\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse, time\n",
        "import math, copy\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageDraw\n",
        "import time\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-gqTCbLkag9B"
      },
      "outputs": [],
      "source": [
        "def show_image(img, scale=1.0):\n",
        "    plt.figure(figsize=scale* plt.figaspect(1))\n",
        "    plt.imshow(img, interpolation='nearest')\n",
        "    plt.gray() \n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2o_cqYRJYvh"
      },
      "source": [
        "# Part 1: SLIC Superpixels (50 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7srjRyqJjgW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Superpixel algorithms group pixels into perceptually meaningful regions while respecting potential object contours, and thereby can replace the rigid pixel grid structure. Due to the reduced complexity, superpixels are becoming popular for various computer vision applications, e.g., multiclass object segmentation, depth estimation, human pose estimation, and object localization.\n",
        "\n",
        "In this problem, you will implement a simple superpixel algorithm called Simple Linear Iterative Clustering (SLIC) that clusters pixels in the five-dimensional color and pixel coordinate space (e.g., r, g, b, x, y). The algorithm starts with a collection of K cluster centers initialized at an equally sampled regular grid on the image of N pixels. For each cluster, you define for a localized window 2S x 2S centered at the cluster center, where S = sqrt(N/K) is the roughly the space between the seed cluster centers. Then, you check whether the pixel within the 2S x 2S local window should be assigned to the cluster center or not (by comparing the distance in 5D space to the cluster center). Once you loop through all the clusters, you can update the cluster center by averaging over the cluster members. Iterate the pixel-to-cluster assignment process till convergence or maximum iterations reached.\n",
        "\n",
        "Reference Paper: http://www.kev-smith.com/papers/SMITH_TPAMI12.pdf\n",
        "\n",
        "You can refer to the following slide covered in Lecture_17_Segmentation.pptx here: https://drive.google.com/file/d/1bRmNBXgK1_kWcY-nhw_As5fP10aRhuDC/view?usp=share_link\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpME_VkbLEAc"
      },
      "source": [
        "### Data\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vva9KDmrLLLk"
      },
      "outputs": [],
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1jbg2VMZ9yAJMHQNRCTgqZM1PyQRtcPyV\n",
        "!unzip \"/content/Part1_SLIC.zip\" -d \"/content/\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRGHHL1lLcWx"
      },
      "source": [
        "### Helper Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "OSa4YF8WgfPf"
      },
      "outputs": [],
      "source": [
        "# A class to initialize the super pixels, of the form - [l,a,b,h,w].\n",
        "class SuperPixel(object):\n",
        "\n",
        "    def __init__(self, l=0, a=0, b=0, h=0, w=0):\n",
        "        self.update(l, a, b, h, w)\n",
        "        self.pixels = []\n",
        "\n",
        "    def update(self, l, a, b, h, w):\n",
        "        self.l = l\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "\n",
        "# Function which returns an object of class SuperPixel\n",
        "def make_SuperPixel(h, w,img):\n",
        "    return SuperPixel(img[h,w][0],img[h,w][1],img[h,w][2], h, w)\n",
        "\n",
        "def display_clusters(img, clusters):\n",
        "    image = np.copy(img)\n",
        "    unq_clusters = []\n",
        "    count = 0\n",
        "    for c in clusters:\n",
        "        for p in c.pixels:\n",
        "            image[p[0],p[1]][0] = c.l\n",
        "            image[p[0],p[1]][1] = c.a\n",
        "            image[p[0],p[1]][2] = c.b\n",
        "        image[int(c.h), int(c.w)][0] = 0\n",
        "        image[int(c.h), int(c.w)][1] = 0\n",
        "        image[int(c.h), int(c.w)][2] = 0\n",
        "        count += 1\n",
        "        cur_cl = [int(c.h), int(c.w)]\n",
        "        if cur_cl not in unq_clusters:\n",
        "          unq_clusters.append(cur_cl)\n",
        "    rgb_arr = color.lab2rgb(image) \n",
        "    show_image(rgb_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFnvOOmOd4ye"
      },
      "source": [
        "### Code (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ZTKsxNJIdy4_"
      },
      "outputs": [],
      "source": [
        "# Functions for Simple Linear Iterative Clustering (SLIC)\n",
        "\n",
        "def initialize_cluster_centers(S, image, img_h, img_w, clusters):\n",
        "    # Initialize \"clusters\" by sampling pixels at an equally sampled regular grid (distanced by S)\n",
        "    # on the image of N pixels. N = img_h X img_w. Hint: clusters.append(make_superPixel(h, w, image)) to create each cluster/super-pixel.\n",
        "    for h in range(1, img_h-1, S):\n",
        "      for w in range(1, img_w-1, S):\n",
        "        clusters.append(make_SuperPixel(h, w, image))\n",
        "    return clusters\n",
        "\n",
        "def relocate_cluster_center_at_lowgrad(clusters, image):\n",
        "    # To Do:\n",
        "    # for each cluster c, reassign cluster to the pixel having smallest gradient value.\n",
        "    # Step 1: compute gradient wrt cluster-center c.h, c.w in 3X3 neighborhood of cluster center.\n",
        "    # Step 2: Similarly, compute gradient for each pixel in 3X3 spatial neighborhood of cluster c.\n",
        "    # Step 3. Reassign cluster-center to the pixel (x,y) having the lowest gradient. \n",
        "    # Hint: c.update(img[x,y][0], img[h,w][1], img[x,y][2], x, y)\n",
        "    gray = image[:,:,0]\n",
        "    for cluster in clusters:\n",
        "      grad_x = gray[cluster.h-1:cluster.h+2, cluster.w-1:cluster.w+2] - gray[cluster.h-1:cluster.h+2, cluster.w:cluster.w+3]\n",
        "      grad_y = gray[cluster.h-1:cluster.h+2, cluster.w-1:cluster.w+2] - gray[cluster.h:cluster.h+3, cluster.w-1:cluster.w+2]\n",
        "      grad = abs(grad_x) + abs(grad_y)\n",
        "      cluster_center = np.argmin(grad)\n",
        "      cluster_center = [cluster_center//3, cluster_center%3]\n",
        "      new_h = cluster.h+cluster_center[0]-1\n",
        "      new_w = cluster.w+cluster_center[1]-1\n",
        "      img_pixel = image[new_h, new_w]\n",
        "      cluster.update(img_pixel[0], img_pixel[1], img_pixel[2], new_h, new_w)\n",
        "    return None\n",
        "\n",
        "# def assign_cluster(clusters, S, image, img_h, img_w, cluster_tag, dis):\n",
        "def assign_cluster(clusters, S, image, img_h, img_w, dis):\n",
        "    # To Do: Compare each pixel to cluster center within 2S pixel distance and assign to nearest cluster using the \"distance metric\"\n",
        "    # (involving both color and spatial dimensions of pixel and cluster, \n",
        "    # Hints: \n",
        "    # 1. use \"dis\" matrix for comparing distances.\n",
        "    # 2. You can use c.pixels.append((h, w)) to keep track of cluster-assignments.\n",
        "    global l_range, a_range, b_range\n",
        "    two_S = 2*S\n",
        "    four_S_sq = two_S**2\n",
        "    for h in range(img_h):\n",
        "      for w in range(img_w):\n",
        "        min_distance = np.inf\n",
        "        selected_cluster = None\n",
        "        for cluster in clusters:\n",
        "          diff_h = h-cluster.h\n",
        "          diff_w = w-cluster.w\n",
        "          if (diff_h**2 + diff_w**2) > four_S_sq:\n",
        "            continue\n",
        "          distance = (abs(diff_h)/img_h + abs(diff_w)/img_w)/2 +\\\n",
        "                     3*(abs(image[h,w][0]-cluster.l)/l_range + abs(image[h,w][1]-cluster.a)/a_range +\\\n",
        "                     abs(image[h,w][2]-cluster.b)/b_range)/3\n",
        "          if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            selected_cluster = cluster\n",
        "        selected_cluster.pixels.append([h, w])\n",
        "    return None\n",
        "\n",
        "def update_clusters(clusters):\n",
        "    # To Do: For each cluster, update the cluster center with mean of the pixels assigned (c.pixels)\n",
        "    for cluster in clusters:\n",
        "      # Get the center location at sub-pixel level.\n",
        "      (new_h, new_w) = np.mean(cluster.pixels, axis=0)\n",
        "      nbd = image[int(new_h//1):int((new_h+2)//1), int(new_w//1):int((new_w+2)//1)]\n",
        "      # Get the lab-value at the sub-pixel level using bilinear interpolation.\n",
        "      new_l = (new_h%1)*((new_w%1)*nbd[1,1,0]+(1-(new_w%1))*nbd[1,0,0]) + (1-(new_h%1))*((new_w%1)*nbd[0,1,0]+(1-(new_w%1))*nbd[0,0,0])\n",
        "      new_a = (new_h%1)*((new_w%1)*nbd[1,1,1]+(1-(new_w%1))*nbd[1,0,1]) + (1-(new_h%1))*((new_w%1)*nbd[0,1,1]+(1-(new_w%1))*nbd[0,0,1])\n",
        "      new_b = (new_h%1)*((new_w%1)*nbd[1,1,2]+(1-(new_w%1))*nbd[1,0,2]) + (1-(new_h%1))*((new_w%1)*nbd[0,1,2]+(1-(new_w%1))*nbd[0,0,2])\n",
        "      cluster.update(new_l, new_a, new_b, new_h, new_w)\n",
        "    return None\n",
        "\n",
        "def compute_res_error(old_clusters, new_clusters):\n",
        "    error = 0.0\n",
        "    # error = Compute L1 distance between previous cluster-centres and new cluster centers.\n",
        "    assert len(old_clusters) == len(new_clusters), 'Cluster sizes ({}&{}) not matching! :('.formta(len(old_clusters),len(new_clusters))\n",
        "    for i in range(len(old_clusters)):\n",
        "      error += abs(old_clusters[i].h-new_clusters[i].h)+abs(old_clusters[i].w-new_clusters[i].w) + \\\n",
        "               abs(old_clusters[i].l-new_clusters[i].l) + abs(old_clusters[i].a-new_clusters[i].a) + abs(old_clusters[i].b-new_clusters[i].b)\n",
        "    return error\n",
        "\n",
        "def slic_algorithm(S, image, img_h, img_w, clusters,dis,k):\n",
        "\n",
        "    clusters = initialize_cluster_centers(S, image, img_h, img_w, clusters)\n",
        "\n",
        "    # Move centers to position in 3x3 window with smallest gradient.\n",
        "    relocate_cluster_center_at_lowgrad(clusters, image)\n",
        "\n",
        "    res_err = 123456789.0 # init residual_error with a very large value (choose as per your understanding.)\n",
        "    threshold = 10000*k/64\n",
        "\n",
        "    iter = 0\n",
        "    iter_times = []\n",
        "    while(1):\n",
        "      start_time = time.time()\n",
        "      iter += 1\n",
        "      assign_cluster(clusters, S, image, img_h, img_w, dis)\n",
        "      old_clusters = copy.deepcopy(clusters)\n",
        "      update_clusters(clusters)\n",
        "      res_err = compute_res_error(old_clusters, clusters)\n",
        "      iter_time = time.time() - start_time      \n",
        "      print('Res Err after {} iters = {}. Last iter took {} s.'.format(iter, res_err, iter_time))\n",
        "      iter_times.append(iter_time)\n",
        "      if (res_err<threshold) or (iter>25):\n",
        "        break\n",
        "\n",
        "    return clusters, iter_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-nHiGIttAxv"
      },
      "outputs": [],
      "source": [
        "all_iter_times = []\n",
        "for img_name in [10081]: #10081, 3063, 70011, 80085, 43033, 20069, 65084, 49024, 70090, 69000\n",
        "  img_path = \"/content/Part1_SLIC/BSD_data/images/{}.jpg\".format(img_name)\n",
        "\n",
        "  # Load image and convert it from an unsigned 8-bit integer to a floating point data type.\n",
        "  image = img_as_float(io.imread(img_path))\n",
        "\n",
        "  # convert RGB to LAB\n",
        "  image = color.rgb2lab(image)\n",
        "  l_range = np.amax(image[:,:,0]) - np.amin(image[:,:,0])\n",
        "  a_range = np.amax(image[:,:,1]) - np.amin(image[:,:,1])\n",
        "  b_range = np.amax(image[:,:,2]) - np.amin(image[:,:,2])\n",
        "  img_h = image.shape[0] #  Height\n",
        "  img_w = image.shape[1] #  Width\n",
        "\n",
        "  # k: Number of clusters/superpixels.\n",
        "  for k in [64]:#, 256, 1024]:\n",
        "    N = img_h * img_w  # Total pixels in the image\n",
        "    S = int(math.sqrt(N /k)) # Average size of each superpixel\n",
        "\n",
        "    clusters = []\n",
        "    dis = np.full((img_h, img_w), np.inf) # Distance bwteen pixels and cluster is initialized as infinity at the beginning.\n",
        "\n",
        "    clusters, iter_times = slic_algorithm(S, image, img_h, img_w, clusters, dis, k)\n",
        "    iter_time = np.mean(iter_times)\n",
        "    print('Mean runtime per iter for K={} for image {} = {}'.format(k, img_name, iter_time))\n",
        "    all_iter_times.append(iter_time)\n",
        "    display_clusters(image, clusters)\n",
        "print('Mean runtime across all iterations across all k values across 10 imgs =', np.mean(all_iter_times))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaWdv8e0L8S9"
      },
      "source": [
        "### Write-up (35 pts)\n",
        "* a) [5 points] Explain your distance function for measuring the similarity between a pixel and cluster in the 5D space.\n",
        "* b) [5 points] Choose one image, try three different weights on the color and spatial feature and show the three segmentation results. Describe what you observe.\n",
        "* c) [5 points] Choose one image, show the error (1) at the initialization and (2) at convergence. Note: error - distance to cluster center in the 5D space.\n",
        "* d) [10 points] Choose one image and show three superpixel results with different number of K, e.g., 64, 256, 1024 and run time for each K.\n",
        "* e)  [10 points] Run your algorithms on the subset (50 images) of Berkeley Segmentation Dataset (BSD) with K = 64, 256 and 1024 and report averaged run-time per image for the BSD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLhVvCQlRa0s"
      },
      "source": [
        "- a) `distance_in_5D = color_weight * (dis_in_l + dis_in_a + dis_in_b)/3 + spatial_weight * (dis_in_h + dis_in_w)/2`   --- (1)\n",
        "  - `dis_in_x` is the normalized distance in the dimension corresponding to `x`. i.e, `dis_in_x` can vary from `0` to `1`.\n",
        "  - Which is achieved by using the following formmula: `Xi = (Xi-min_X) / (max_X-min_X)`\n",
        "  - Finally, the expression in (1) calculates distance by giving custom weights to the spatial & color features (More in next section of the write-up). \n",
        "  - Since, there are `3` color dimensions, the sum of the distances in color dimensions is divided by `3`. Similarly, the sum of the distances in spatial dimensions is divided by `2` - so that if same value is chosen for `color_weight` & `spatial_weight`, color & spatial features are weighted equally.\n",
        "- b) Three different weights are experimented with (k=64 for all the below imgs):\n",
        "  - (i)  Higher weight to color features (`color_weight=2` & `spatial_weight=1`):\n",
        "    - <img src=\"https://drive.google.com/uc?id=1wh-pakEeC30ZlSm6avfiCnoDitW-ci7f\" width=\"400\"/>\n",
        "  - (ii) Higher weight to spatial features (`color_weight=1` & `spatial_weight=2`):\n",
        "    - <img src=\"https://drive.google.com/uc?id=17uLylnz-bEwtVHR5jakx86dIFU_Qq0Sg\" width=\"400\"/>\n",
        "  - (ii)   Equal weights (`color_weight=1` & `spatial_weight=1`):\n",
        "    - <img src=\"https://drive.google.com/uc?id=1jvB6UyIwyiBaocwl79ttJuKLsKUOIaAD\" width=\"400\"/>\n",
        "  \n",
        "  - <u>OBSERVATIONS</u>:\n",
        "    - If higher weight is given to spatial features, nearby pixels are getting clustered, without a lot of consideration to the color-similarity of the pixels. The resultant image has bigger super-pixel for each color & some colors are vanishing as well. If the weight for spatial features is further increased, more & more colors are expected to vanish.\n",
        "    - Higher weight to color features caused the super-pixels to be smaller in size and show more of the true colors. If the weight for color features is further increased, super pixels are expected to get even smaller & more true colors are expected to be retained.\n",
        "    - Finally, equal weights to spatial & color features finds the middle ground between both.\n",
        "\n",
        "- c) For the image 10081.jpg for `k=64` and with equal weights to spatial & color features, the change in cluster coordinates in the 5D space due to the 1st iteration of the algo & due to the last iteration are `1796.94` & `31.19` respectively. This shows that the algorithm is converging.\n",
        "- d) The following are the superpixel results along with average run-time per iteration for the image 10081.jpg with:\n",
        "  - `k=64` (runtime per iteration = `31.14` s):\n",
        "    - <img src=\"https://drive.google.com/uc?id=1jvB6UyIwyiBaocwl79ttJuKLsKUOIaAD\" width=\"400\"/>\n",
        "  - `k=256` (runtime per iteration = `68.12` s):\n",
        "    - <img src=\"https://drive.google.com/uc?id=1PxAyy6YS8TIFfmibENXXcapfgsSEPXBM\" width=\"400\"/>\n",
        "  - `k=1024` (runtime per iteration = `225.6` s):\n",
        "    - <img src=\"https://drive.google.com/uc?id=1YSIJ5WKUqdtlgnJOYtQp5ytFumTbuGPK\" width=\"400\"/>\n",
        "\n",
        "- e) The average runtime per image per iteration is `104.07` s.\n",
        "- The average runtime per image per iteration for:\n",
        "  - `k=64`   is `29.2` s\n",
        "  - `k=256`  is `66.4` s\n",
        "  - `k=1024` is `216.6` s\n",
        "\n",
        "\n",
        "\n",
        "NOTE:\n",
        "1) I am updating cluster centers at sub-pixel level after every iteration and getting the pixel values using BILINEAR INTERPOLATION."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Z_uRsrOhSK"
      },
      "source": [
        "### Extra Credit (20 pts)\n",
        "* f) [upto 10 points] Implement and report two types of metrics (1) boundary recall and (2) under-segmentation error with K = 64, 256 and 1024 on the BSD (last part of write-up).\n",
        "* g) [upto 10 points] Try to improve your result from first part of extra-credit. You may try different color space (e.g., CIELab, HSV) (See Sec 4.5 in the paper), richer image features (e.g., gradients) or any other ideas you come up with. Report the accuracy on boundary recall and under-segmentation error with K = 256. Compare the results with first-part of extra-credit and explain why you get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqvFt0VTUqZN"
      },
      "source": [
        "### Hints \n",
        "- For main part, you CAN NOT use any library such as skimage / cv2 to perform or implement SLIC segmentation.\n",
        "- For better visualization, you may use external function (skimage.segmentation.mark_boundaries) for your purpose.\n",
        "- For EXTRA CREDIT part, you are allowed to use external library for metrics (boundary recall and under-segmentation error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNbcmI_JPV4x"
      },
      "source": [
        "# Part 2: Graph-cut Segmentation (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8YEx5qcP6c1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Let us apply Graph-cuts for foreground/background segmentation. In the “cat” image, you are given a rough polygon of a foreground cat. Apply graph-cut based method to see if we can get a better segmentation!\n",
        "\n",
        "Firstly, use the provided polygon to obtain an estimate of foreground and background color likelihood. You may choose the likelihood distribution (e.g., color histograms or color mixture of Gaussians.).\n",
        "\n",
        "Secondly, apply graph-cut code (cv2.grabcut) to do better segmentation. You can use the provided bounding-boxes and use \"cv2 package\" for the implementation sake. You are required to draw interesting conclusions on how it performs on different samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-acBTxIW09h"
      },
      "source": [
        "## Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tIlqOFVIOHay",
        "outputId": "0ed93274-97a5-4520-d9b0-bc8c34cf01fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Tw_gk0yylwl2X1leubz0mlERpxsqLiub\n",
            "To: /content/Part2_GraphCut.zip\n",
            "\r  0% 0.00/264k [00:00<?, ?B/s]\r100% 264k/264k [00:00<00:00, 90.6MB/s]\n",
            "Archive:  /content/Part2_GraphCut.zip\n",
            "   creating: /content/Part2_GraphCut/\n",
            "  inflating: /content/Part2_GraphCut/dog.jpg  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._dog.jpg  \n",
            "  inflating: /content/Part2_GraphCut/messi.jpg  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._messi.jpg  \n",
            "  inflating: /content/Part2_GraphCut/.DS_Store  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._.DS_Store  \n",
            "  inflating: /content/Part2_GraphCut/plane.jpg  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._plane.jpg  \n",
            "  inflating: /content/Part2_GraphCut/bird.jpg  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._bird.jpg  \n",
            "  inflating: /content/Part2_GraphCut/cat.jpg  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._cat.jpg  \n",
            "  inflating: /content/Part2_GraphCut/cat_poly.mat  \n",
            "  inflating: /content/__MACOSX/Part2_GraphCut/._cat_poly.mat  \n"
          ]
        }
      ],
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "# !gdown 1ObpNoshjKMcB7SFvrTuoKe61IE-HG95n\n",
        "!gdown 1Tw_gk0yylwl2X1leubz0mlERpxsqLiub\n",
        "!unzip \"/content/Part2_GraphCut.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHCEJKXOArZG"
      },
      "source": [
        "## Code (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HU25gtAxjTG6"
      },
      "outputs": [],
      "source": [
        "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
        "    fill_row_coords, fill_col_coords = draw.polygon(vertex_row_coords, vertex_col_coords, shape)\n",
        "    mask = np.zeros(shape, dtype=np.bool)\n",
        "    mask[fill_row_coords, fill_col_coords] = True\n",
        "    return mask\n",
        "\n",
        "# Get a normalized version of the given histograms (divide by sum)\n",
        "def normalize_histograms(histograms):\n",
        "  return np.float32([h / h.sum() for h in histograms])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "FyWFpBzTwmw8"
      },
      "outputs": [],
      "source": [
        "def mask_for_fg(img, img_mask, fg_image):\n",
        "  # to generate foreground region (and everything else masked out).\n",
        "  fg_image = np.zeros_like(img)\n",
        "  fg_image[img_mask] = img[img_mask]\n",
        "  return fg_image\n",
        "\n",
        "def mask_for_bg(img, img_mask):\n",
        "  # generate background region (and everything else masked out).\n",
        "  bg_image = np.zeros_like(img)\n",
        "  bg_image[img_mask] = img[img_mask]\n",
        "  return bg_image\n",
        "\n",
        "def visualize_likelihood_map(gmm_bg, gmm_fg, img, num_gaussians):\n",
        "  reshaped_img = img.reshape((-1,3))\n",
        "  log_likelihood_bg = -gmm_bg.score_samples(reshaped_img).reshape((img.shape[0], -1))\n",
        "  log_likelihood_fg = -gmm_fg.score_samples(reshaped_img).reshape((img.shape[0], -1))\n",
        "  x=np.array([i for i in range(log_likelihood_fg.shape[1])])\n",
        "  y=[i for i in range(log_likelihood_fg.shape[0])]\n",
        "  y.reverse()\n",
        "  y=np.array(y)\n",
        "  fig = go.Figure(data=go.Heatmap(x=x, y=y, z=log_likelihood_fg, text=np.around(log_likelihood_fg, 2).astype('<U4'), texttemplate=\"%{text}\", textfont={'color':'black'}))\n",
        "  fig.update_layout(title=dict(text='NLL map for fg with a GMM of {} Gaussians'.format(num_gaussians), x=0.5), width=img.shape[1], height=img.shape[0], \\\n",
        "                    xaxis_visible=False, xaxis_showticklabels=False, yaxis_visible=False, yaxis_showticklabels=False)\n",
        "  fig.show()\n",
        "  fig = go.Figure(data=go.Heatmap(x=x, y=y, z=log_likelihood_bg, text=np.around(log_likelihood_bg, 2).astype('<U4'), texttemplate=\"%{text}\", textfont={'color':'black'}))\n",
        "  fig.update_layout(title=dict(text='NLL map for bg with a GMM of {} Gaussians'.format(num_gaussians), x=0.5), width=img.shape[1], height=img.shape[0], \\\n",
        "                    xaxis_visible=False, xaxis_showticklabels=False, yaxis_visible=False, yaxis_showticklabels=False)\n",
        "  fig.show()\n",
        "  return None\n",
        "\n",
        "def gaussian_model(img, num_gaussians):\n",
        "  # fit gaussian model on a given image.\n",
        "  reshaped_img = img.reshape((-1,3))\n",
        "  gmm_model = GaussianMixture(num_gaussians, random_state=2)\n",
        "  gmm_labels = gmm_model.fit_predict(reshaped_img)\n",
        "  return gmm_model, gmm_labels\n",
        "\n",
        "def run_grabcut(img, bbox):\n",
        "  # You can use opencv cv2.grabCut algorithm with \"cv2.GC_INIT_WITH_RECT\" to implement this.\n",
        "  grabcut_mask = np.zeros(img.shape[:2],np.uint8)\n",
        "  grabcut_mask[bbox[0]:bbox[0]+bbox[2], bbox[1]:bbox[1]+bbox[3]] = 1\n",
        "  rect = (bbox[1], bbox[0], bbox[3], bbox[2])\n",
        "  bgModel = np.zeros((1, 65), np.float64)\n",
        "  fgModel = np.zeros((1, 65), np.float64)\n",
        "  grabcut_mask, bgModel, fgModel = cv2.grabCut(img, grabcut_mask, rect, None, None, 1000, cv2.GC_INIT_WITH_RECT)\n",
        "  grabcut_mask = np.where((grabcut_mask == 2)|(grabcut_mask == 0), 0, 1).astype('uint8')\n",
        "  return (grabcut_mask, bgModel, fgModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "u8yMhvEhYiyw",
        "outputId": "b59a6033-73ca-4fb2-9104-e9a0c86fd2f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning:\n",
            "\n",
            "`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Main Block: ###\n",
        "\n",
        "# read image\n",
        "img_name = 'cat'\n",
        "img_path = '/content/Part2_GraphCut/cat.jpg'\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# read image_mask\n",
        "poly = scipy.io.loadmat('/content/Part2_GraphCut/cat_poly.mat')['poly']\n",
        "img_mask = poly2mask(poly[:,1], poly[:,0], (img.shape[0],img.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "jLgabcLCHniI"
      },
      "outputs": [],
      "source": [
        "# 1. Fit Gaussian mixture model for foreground regions.\n",
        "\n",
        "# Generate image (fg_image) with background masked using img_mask\n",
        "fg_image = None\n",
        "fg_image = mask_for_fg(img, img_mask, fg_image)\n",
        "num_gaussians = 18\n",
        "gmm_fg, fg_gmm_labels = gaussian_model(fg_image, num_gaussians)\n",
        "\n",
        "\n",
        "# 2. Fit Gaussian mixture model for background regions.\n",
        "# Generate image (fg_image) with foreground masked using img_mask\n",
        "bg_image = mask_for_bg(img, ~img_mask)\n",
        "gmm_bg, bg_gmm_labels = gaussian_model(bg_image, num_gaussians)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUe5R9am17fM"
      },
      "outputs": [],
      "source": [
        "# 3. Compute and visualize the foreground cost and background cost as an image.\n",
        "# Foreground label cost: -log Pr[Image | foreground model]\n",
        "# Foreground label cost: -log Pr[Image | background model]\n",
        "visualize_likelihood_map(gmm_bg, gmm_fg, img, num_gaussians)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVfZqlh04dkT"
      },
      "outputs": [],
      "source": [
        "# 4. Run grabcut algorithm.\n",
        "bboxes = {'cat':(80,80,320,250), 'dog':(50,300,600,300), 'plane': (180,80,250,800), 'messi':(10,80,600,500), 'bird':(100,100,350,250)}\n",
        "bboxes = {'cat':(80,80,320,250)}\n",
        "for img_name in bboxes.keys():\n",
        "  img_path = '/content/Part2_GraphCut/{}.jpg'.format(img_name)\n",
        "  img = io.imread(img_path)\n",
        "  bbox = bboxes[img_name]\n",
        "  (grabcut_mask, bgModel, fgModel) = run_grabcut(img, bbox)\n",
        "  show_image(grabcut_mask)\n",
        "  masked_img = img*grabcut_mask[:,:,np.newaxis]\n",
        "  show_image(masked_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwA-yyok-9Z5"
      },
      "source": [
        "### Write-up (35 points)\n",
        "\n",
        "- a) [5 points] Explain your foreground and background likelihood function.\n",
        "- b) [10 points] Your foreground and background likelihood map. \n",
        "\n",
        "Display P(foreground|pixel) as an intensity map (bright = confident foreground).\n",
        "- c) [15 points] Implement grabcut segmentation using cv2 package and draw insights on how the segmentation performs on different sample images (hard v/s easy).\n",
        "- d) [5 points] Try grabcut algorithm with different \"iterCount\" to see if results improve in certain cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RVnA0lP3n-W"
      },
      "source": [
        "- a) I modelled the foreground & background pixels separately as a mixture of n-Gaussians (I tried various values for `n`) using the polygons provided for the ground truth of cat. This is the Maximum Likelihood Estimation step.\n",
        "  - The RGB values of each pixel are used to form a 3D feature vector for each pixel. This feature vector is used for the said modelling.\n",
        "- b) Then, the obtained models for the foreground & the background are used to perform probabilistic inference to generate the probability that a pixel belongs either to the foreground or to the background.\n",
        "  - Later on, negative of log is applied on these probabilities to obtain the final maps.\n",
        "  - The below images show the Negative log-likelihood (NLL) maps for foreground & background when different number of gaussians are used for modelling.\n",
        "  - <img src=\"https://drive.google.com/uc?id=1wAFDZMl7iwoyvQFcGkP-ig6NdMzdudnM\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=1LBtD0Xc5l_cjJKsPpxc55b8AppfzFz-z\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=1-G9TzrKxDPsipn-qjCsDaRQG5NctPu0Z\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=18rrsM66nqf3z2WNcG4gBadEppdzWVxRq\" width=\"400\"/>\n",
        "  - <u> NOTE: </u>\n",
        "    - Since these are NLL maps, smaller value (darker color) means higher probability.\n",
        "  - <u> OBSERVATIONS: </u>\n",
        "    - The sky-blue colored windowsill is predicted as background more confidently when the background is modelled using 5 Gaussians.\n",
        "  - <u> INFERENCE: </u>\n",
        "    - The distribution of pixel intensities of the background is multi-modal with at least 5 modes.\n",
        "\n",
        "- c) Grabcut Segmentation:\n",
        "  - <img src=\"https://drive.google.com/uc?id=1ZOG3bxXikTX3Mc6HWFQGavVhlW7yTLst\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=1hgz_Yt3l44RQ5mFZrbbhrvjztVXn_cJE\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=1qoFxpw8Lo8szqOAYqPAb6nNwyQxecSQu\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=1HgQF3tWyQF6VT7wBZ_bHNN19j0MyYyga\" width=\"400\"/>\n",
        "  - <img src=\"https://drive.google.com/uc?id=1rH-irMgizbpVpR94RQAdEzg0EslNxPtU\" width=\"400\"/>\n",
        "  - <u>NOTE1:</u>\n",
        "    - messi.jpg is an easy photo as the foreground is distinclty different from the background. But, as can be seen in the above segmentation result, Messi's left palm didn't get segmented-out even though the palm has similarities with those parts of the foreground that did get segmented out. This is due to the provided window (the one with width=500).\n",
        "    - The window is supposed to cover the foreground object completely (as the algorithm considers everything which lies outside the window as background) but the provided window didn't do so (as can be seen below):\n",
        "    - <img src=\"https://drive.google.com/uc?id=1PC8enrtTGHd-mRgyXHhBgcxTeQuiQN-b\" width=\"400\"/>\n",
        "    - Instead, if the window passed to the `grab-cut` function completely covers the foreground object, the left palm did get segmented out. The below images are the rectified window & the segmentation result using the rectified window.\n",
        "    - <img src=\"https://drive.google.com/uc?id=1KNTGVq7UxWLC2T8g1XYVYpM1524APz87\" width=\"400\"/>\n",
        "    - <img src=\"https://drive.google.com/uc?id=1uKNimq5SuJ_xyMrQTgAtvZDiVkbBzqw7\" width=\"400\"/>\n",
        "    - Further, it can be observed in the above image that the grass between the left fingers has a brownish hue (unlike the rest of the background) and consequently got segmented as foreground. And, the grass between right fingers has a green hue (like the rest of the background) and didn't get segmented-out even though it's a very small patch.\n",
        "  - <u>NOTE2:</u>\n",
        "    - A similar argument can be made about the cat image in which the brick wall has a similar color distribution as the fur of the cat & hence it got segmented out.\n",
        "    - But, there's <i>some</i> distinction between the color distributions and it can be expected that the brick wall can be segmented away as background with a little feedback to the grab-cut function.\n",
        "  - <u>NOTE3:</u>\n",
        "    - The segmentation of the plane image is noisy because the image is blurry.\n",
        "  - d) Improvement with more iterations:\n",
        "    - The above results are generated for `iterCount=5`. The below image is the segmentation result for running with `iterCount=1000`.\n",
        "    - <img src=\"https://drive.google.com/uc?id=16FR8ipLJg6Tj2XEod0kA-dFeTp3bv0OG\" width=\"400\"/>\n",
        "    - As can be observed, this result is cleaner than the previous result. Particularly, the edges of the windowsill are not included in the result & the edges of the cat's ears got rid of the noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ9_VvCZVRpz"
      },
      "source": [
        "### Hints \n",
        "- You may refer to https://docs.opencv.org/4.x/dd/dfc/tutorial_js_grabcut.html to implement run_grabcut()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "H2o_cqYRJYvh",
        "h-acBTxIW09h"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
