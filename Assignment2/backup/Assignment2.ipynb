{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_Iqu-qnIj3"
      },
      "outputs": [],
      "source": [
        "## Import necessary libraries here (You can add libraries you want to use here)\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftoiwAo45n5"
      },
      "source": [
        "# Part 1: A Feature Tracker (50 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJikaY9ICDYH"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In the problem, you will implement a corner detector and feature tracker that track features from the image sequence hotel. Since this is a two part problem, we have included precomputed intermediate results in the *Data* section in case youâ€™re unable to complete any portion.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1sBtKpU2mYEPZ9c2Cvw-DBuLBPK2gYwC-\" width=\"700\"/>\n",
        "\n",
        "**Note:**  Do not use existing keypoint\n",
        "detectors, trackers, or structure from motion code, such as found on the web, and OpenCV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR0sjl-f6qkm"
      },
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uWzKA6i68ls"
      },
      "outputs": [],
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "if not os.path.exists('/content/part1_images/hotel.seq41.png'):\n",
        "  !gdown 1fT0H-FbbDZnjMfCJHZcscpcwAXHhGgNw\n",
        "  !unzip \"/content/part1_images.zip\" -d \"/content/\"\n",
        "  !gdown 1r-Pdino6MRLCEWX_sQOgd8D5AVsRc7Ym\n",
        "  # Load Initial Key Points\n",
        "  data = loadmat('/content/initial_keypoints.mat')\n",
        "  X0 = data['Xo']\n",
        "  Y0 = data['Yo']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Pd1s5uL2Bg"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gESONvcFL4y0"
      },
      "outputs": [],
      "source": [
        "def readImages(folder, num_images):\n",
        "  arr_images = []\n",
        "  for i in range(num_images):\n",
        "    arr_images.append(cv2.imread(f'{folder}hotel.seq{i}.png'))\n",
        "  return np.array(arr_images, dtype=np.float32)\n",
        "\n",
        "def compute_H(patch): #Given a patch, compute the 2nd moment matrix within the patch\n",
        "  Ix = patch[:, 1:] - patch[:, :-1]\n",
        "  Iy = patch[1:, :] - patch[:-1, :]\n",
        "  Ix = Ix[:-1, :]\n",
        "  Iy = Iy[:, :-1]\n",
        "  Ix_sq = np.sum(np.square(Ix))\n",
        "  Iy_sq = np.sum(np.square(Iy))\n",
        "  Ix_Iy = np.sum(Ix * Iy)\n",
        "  H = np.array([[Ix_sq, Ix_Iy], [Ix_Iy, Iy_sq]])\n",
        "  return H, Ix, Iy\n",
        "\n",
        "def pad_the_patch(patch): #pad the image by replicating the last row & last column. Resultant patch will have 1 additional row & column than the original one.\n",
        "  patch = np.concatenate((patch, np.expand_dims(patch[:, -1], axis=1)), axis=1)\n",
        "  patch = np.concatenate((patch, np.expand_dims(patch[-1, :], axis=0)), axis=0)\n",
        "  return patch\n",
        "\n",
        "# read all 51 sequences of images\n",
        "folder = '/content/part1_images/'\n",
        "im = readImages(folder, 51)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ouOF_UlTKF"
      },
      "source": [
        "## 1.1 Keypoint Selection (15 pts)\n",
        "\n",
        "For the first frame, use the second moment matrix to locate strong corners to use as keypoints.\n",
        "These points will be tracked throughout the sequence in the second part of the problem. Choose a proper threshold so that edges and noisy patches are ignored. Do local non-maxima suppression over a 5x5 window centered at each point.\n",
        "This should give several hundred good points to track."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efy0NyPjo3y3"
      },
      "source": [
        "### Code (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeXJU9eHmzMb"
      },
      "outputs": [],
      "source": [
        "def apply_NMS(f_values, window_size):\n",
        "  '''\n",
        "  Apply local Non-Maxima Suppression:\n",
        "  1) Slide a 5x5 window all over the image.\n",
        "  2) Retain the center of those 5x5 windows as keypoints in which the center pixel is the local maxima in that window.\n",
        "  '''\n",
        "  (rows, cols) = f_values.shape\n",
        "  half_size = window_size//2\n",
        "  keypoints = np.zeros((rows, cols))\n",
        "  for i in range(half_size, rows-half_size):\n",
        "    for j in range(half_size, cols-half_size):\n",
        "      patch = f_values[i-half_size:i+half_size+1, j-half_size:j+half_size+1]\n",
        "      uniq, counts = np.unique(patch, return_counts=True)\n",
        "      if counts[-1] > 1:\n",
        "        continue\n",
        "      if uniq[-1] == f_values[i, j]:\n",
        "        keypoints[i, j] = 255\n",
        "  return keypoints\n",
        "\n",
        "def getKeypoints(img, tau):\n",
        "  '''\n",
        "  Detecting keypoints using Harris corner criterion\n",
        "  img: input image\n",
        "  tau: threshold \n",
        "  \n",
        "  output: (N,2) array of [x,y] keypoints (x & y are 0-indexed with upper left pixel of the image being (0,0). i.e, OpenCV frame convention).\n",
        "  '''\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  (rows, cols) = img.shape\n",
        "  window_size = 5 \n",
        "  half_size = window_size//2\n",
        "  f_values = np.zeros_like(img)\n",
        "  for i in range(half_size, rows-half_size):\n",
        "    for j in range(half_size, cols-half_size):\n",
        "      patch = img[i-half_size:i+half_size+1, j-half_size:j+half_size+1]\n",
        "      padded_patch = pad_the_patch(patch)\n",
        "      H, _, _ = compute_H(padded_patch)\n",
        "      harris_operator = np.linalg.det(H) / np.trace(H)\n",
        "      if harris_operator>tau:\n",
        "        f_values[i, j] = harris_operator\n",
        "  f_values = f_values/np.amax(f_values)\n",
        "  keypoints = apply_NMS(f_values, window_size)\n",
        "  print('Number of keypoints selected:', np.sum(keypoints)/255)\n",
        "\n",
        "  keypoints_opencv = []\n",
        "  for i in range(half_size, rows-half_size):\n",
        "    for j in range(half_size, cols-half_size):\n",
        "      if keypoints[i,j] == 255:\n",
        "        keypoints_opencv.append([j, i])\n",
        "  return keypoints_opencv\n",
        "\n",
        "tau = 3.15*255\n",
        "# tau is tuned as below: <tau> -> <number of keypoints it gave>\n",
        "# 0.17->1102 0.25->1061 0.55->991 0.85->865 0.95->813 1.15->740 3.15->201\n",
        "keypoints_opencv = getKeypoints(im[0], tau)\n",
        "\n",
        "# add plots for the write-up\n",
        "display_img = cv2.cvtColor(im[0], cv2.COLOR_BGR2GRAY)\n",
        "(rows, cols) = display_img.shape\n",
        "display_img = cv2.cvtColor(display_img, cv2.COLOR_GRAY2BGR)\n",
        "for key_pt in keypoints_opencv:\n",
        "  display_img = cv2.circle(display_img, (key_pt[0], key_pt[1]), radius=3, color=(0, 255, 0), thickness=-1)\n",
        "cv2_imshow(display_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmN8MtK2mOUz"
      },
      "source": [
        "### Write-up (10 pts)\n",
        "\n",
        "\n",
        "*   (5 pts) Explain your implementation of getKeypoints()\n",
        "*   (5 pts) Display the first frame of the sequence overlaid with the detected keypoints. Ensure that they are clearly visible (plot with `color='g' and linewidths=3`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I06DhqYI4YI7"
      },
      "source": [
        "#### Description of getKeypoints():\n",
        "- A 5x5 window is slid on the grayscale image (with a stride of 1) & the 2nd moment matrix `H` is calculated using the below formula:\n",
        "\n",
        "  - <img src=\"https://drive.google.com/uc?id=1_W_a9FwMjDlyTWAXLOrXvsFBcIEeZfli\" height=400 align=\"center\"/>\n",
        "  - where:\n",
        "    - `i_x` & `i_y` are the gradients in x & y directions.\n",
        "    - The gradient `i_x` is calculated as `intensity of pixel (p+1,q) - intensity of pixel (p,q)`\n",
        "    - The summations in the formulas for `A`,`B`&`C` run over the 5x5 window.\n",
        "\n",
        "- The harris operator is calculated by the below formula:\n",
        "  - <img src=\"https://drive.google.com/uc?id=1gBxEZQ1yttjpwMHdpNpbFhq4kZlQJPUr\" height=200 align=\"center\"/>\n",
        "  - It is calculated over all the 5x5 windows.\n",
        "- ##### Thresholding:\n",
        "  - Those pixels for which harris operator yielded a value of greater than `tau` were marked as potential keypoints & the rest are discarded.\n",
        "  - `tau = 803.25` was selected after a bit of tuning.\n",
        "- ##### Normalization:\n",
        "  - Then, the resultant harris operator matrix (of the same size as image) is divided by its maximum value.\n",
        "- Finally, non-maxima suppression is done (in the below way):\n",
        "  - If a pixel is strictly greater than every other pixel in the 5x5 neighborhood centered around it, it is retained. Else, it is discarded.\n",
        "- Note:\n",
        "  - No padding is done on the original image.\n",
        "  - Due to this, no keypoints will be detected in the `window_size//2` = 2 columns or rows along the border.\n",
        "\n",
        "#### Keypoints:\n",
        "- The below image shows the final keypoints obtained by following the above method.\n",
        "- `201` keypoints were selected for `tau = 803.25`.\n",
        "- <img src=\"https://drive.google.com/uc?id=1thCSU_TWlgDcGmzrcDMjZ43CszK2cQl3\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0-XMmadpALk"
      },
      "source": [
        "## 1.2 Feature Tracking (35 pts)\n",
        "\n",
        "Apply the Kanade-Lucas-Tomasi tracking procedure to track the keypoints found in part 1.1 (or the given keypoints in the *Data* section) throughout the hotel sequence. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1dU4p4YcnXoQFnrNvleEty_4tDECkVW9Q\" width=\"500\"/>\n",
        "\n",
        "Some keypoints will move out of the image frame over the course of the sequence. Discard any track if the predicted translation falls outside the image frame.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "*  From the 1st frame to the 2nd frame, use the detected keypoints at the first frame as initialization points. From the 2nd to 3rd frame, use the tracked positions at the 2nd frame as initialization. Note that the tracked keypoints in general are not at integer positions.\n",
        "\n",
        "*  For each point, use a window size of 15 x 15.\n",
        "\n",
        "Add codes to **plot** your tracked points overlayed in the **first sequence** and the **last sequence**. They should look similar to the second picture shown in the Overview section. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J0iUZg6vFee"
      },
      "source": [
        "### Code (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnjkdkuWtzo9"
      },
      "outputs": [],
      "source": [
        "def trackPoints(pt_x, pt_y, im, ws, out_of_bound_pts):\n",
        "  '''\n",
        "  Tracking initial points (pt_x, pt_y) across the image sequence\n",
        "  Outputs:\n",
        "    track_x: [Number of keypoints] x [2]\n",
        "    track_y: [Number of keypoints] x [2]\n",
        "  '''\n",
        "  pt_x = pt_x.astype('float32')\n",
        "  pt_y = pt_y.astype('float32')\n",
        "  N = np.prod(pt_x.shape)\n",
        "  nim = len(im)\n",
        "  track_x = np.zeros((N, nim))\n",
        "  track_y = np.zeros((N, nim))\n",
        "  track_x[:,0] = pt_x\n",
        "  track_y[:,0] = pt_y\n",
        "  if len(im.shape) == 4:\n",
        "    im = im[:, :, :, 0].astype('float32')\n",
        "  else:\n",
        "    input('No resizing is being done. Continue?')\n",
        "  for t in range(nim-1):\n",
        "    track_x[:, t+1], track_y[:, t+1] = getNextPoints(track_x[:, t], track_y[:, t], im[t,:,:], im[t+1,:,:], ws, out_of_bound_pts)\n",
        "\n",
        "  return np.concatenate((np.expand_dims(track_x, axis=2), np.expand_dims(track_y, axis=2)), axis=2)\n",
        "\n",
        "def getNextPoints(xs, ys, im1, im2, ws, out_of_bound_pts):\n",
        "  '''\n",
        "  Iterative Lucas-Kanade feature tracking\n",
        "  x,  y : initialized keypoint position in im2\n",
        "  ws: patch window size\n",
        "\n",
        "  output: tracked keypoint positions in im2\n",
        "  '''\n",
        "  threshold = 0.5\n",
        "  x_next_frame = []\n",
        "  y_next_frame = []\n",
        "  (rows, cols) = im1.shape\n",
        "  for idx, (x,y) in enumerate(zip(xs, ys)):\n",
        "    u = 10\n",
        "    v = 10\n",
        "    x_dash = copy.copy(x)\n",
        "    y_dash = copy.copy(y)\n",
        "    patch = cv2.getRectSubPix(im1, (ws,ws), (x,y))\n",
        "    H, Ix, Iy = compute_H(pad_the_patch(patch))\n",
        "    while (u>threshold) or (v>threshold):\n",
        "      It = cv2.getRectSubPix(im2, (ws,ws), (x_dash,y_dash)) - cv2.getRectSubPix(im1, (ws,ws), (x,y))\n",
        "      Ix_It = -1 * (np.sum(Ix*It))\n",
        "      Iy_It = -1 * (np.sum(Iy*It))\n",
        "      b = [Ix_It, Iy_It] # To solve ax=b, a=H & b is this matrix\n",
        "      try:\n",
        "        [u, v] = np.linalg.solve(H, b)\n",
        "      except:\n",
        "        print(np.linalg.det(H))\n",
        "        input('stopping')\n",
        "      new_x_dash = x_dash + u\n",
        "      new_y_dash = y_dash + v\n",
        "      if new_y_dash<0 or new_y_dash>rows-1 or new_x_dash<0 or new_x_dash>cols-1:\n",
        "        out_of_bound_pts.append(idx)\n",
        "        break\n",
        "      else:\n",
        "        x_dash = new_x_dash\n",
        "        y_dash = new_y_dash\n",
        "    x_next_frame.append(x_dash)\n",
        "    y_next_frame.append(y_dash)\n",
        "  return x_next_frame, y_next_frame\n",
        "\n",
        "ws = 7\n",
        "keypoints_list = copy.copy(np.array(keypoints_opencv))\n",
        "out_of_bound_pts = []\n",
        "tracked_pts = trackPoints(pt_x=keypoints_list[:,0], pt_y=keypoints_list[:,1], im=im, ws=ws, out_of_bound_pts=out_of_bound_pts)\n",
        "\n",
        "# plot your results\n",
        "display_img1 = copy.copy(im[0])\n",
        "display_img2 = copy.copy(im[0])\n",
        "display_img3 = copy.copy(im[0])\n",
        "rand_20_tracked_pts = np.random.permutation(np.arange(tracked_pts.shape[0]))[:20]\n",
        "for i in range(tracked_pts.shape[0]):\n",
        "  tracked_pt = tracked_pts[i]\n",
        "  display_img1 = cv2.circle(display_img1, (int(tracked_pt[0][0]), int(tracked_pt[0][1])), radius=2, color=(0, 255, 0), thickness=-1)\n",
        "  display_img1 = cv2.circle(display_img1, (int(tracked_pt[1][0]), int(tracked_pt[1][1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
        "  if i in rand_20_tracked_pts:\n",
        "    for consec_frame_pts in tracked_pt:\n",
        "      display_img2 = cv2.circle(display_img2, (int(consec_frame_pts[0]), int(consec_frame_pts[1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
        "  if i in out_of_bound_pts:\n",
        "    for consec_frame_pts in tracked_pt:\n",
        "      display_img3 = cv2.circle(display_img3, (int(consec_frame_pts[0]), int(consec_frame_pts[1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "cv2_imshow(display_img1)\n",
        "cv2_imshow(display_img2)\n",
        "cv2_imshow(display_img3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O76508F1vH0_"
      },
      "source": [
        "### Write-up (25 pts)\n",
        "\n",
        "*   (5 pts) For all the keypoints, display (1) the keypoints at the first frame (as green) and (2) the tracked keypoints at the second frame (as red) on the first frame of the sequence.\n",
        "*   (10 pts) For 20 random keypoints, draw the 2D path over the sequence of frames. That is, plot the progression of image coordinates for each of the 20 keypoints. Plot each of the paths on the same figure, overlaid on the first frame of the sequence.\n",
        "*   (10 pts) On top of the first frame, plot the points which have moved out of frame at some point along the sequence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHYhHqAM9R1l"
      },
      "source": [
        "- <img src=\"https://drive.google.com/uc?id=1kz8e8M1_RTHjEpZVAtdn8rew49lDUsbI\" height=400 align=\"center\"/>\n",
        "\n",
        "- <img src=\"https://drive.google.com/uc?id=1SbVyEFGKRlysslutuPNzkoj2rB0Y12c1\" height=400 align=\"center\"/>\n",
        "\n",
        "- <img src=\"https://drive.google.com/uc?id=1aRwEDLJ39ZgMVjPk8F1xHjHQ5UZEzRMG\" height=400 align=\"center\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UurFy8hB5BeP"
      },
      "source": [
        "# Part 2: Shape Alignment (30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baagTPK5aa9r"
      },
      "source": [
        "## Overview\n",
        "In this problem, you will write a function that aligns two sets of points using global image transformation (similarity, affine, or perspective) and returns $T$  where $T$ is a transformation that maps non-zero points in $im1$ to non-zero points in $im2$. You may choose the alignment algorithm and the type of (global) transformation (e.g., rigid Euclidean, affine, perspective).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1PnWIy9ZdP9SGkmGNtFCJ-JzKLwmW-qaN\" width=\"1000\"/>\n",
        "\n",
        "Test your code on the 25 image pairs provided in the supplementary material. We have included functions \n",
        "**(will check) evalAlignmentAll and displayAlignment to help with evaluation and display**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWCfxrUA7LNb"
      },
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5aF6Biq7LvQ",
        "outputId": "775f8a5a-55d3-4ca0-ea4d-cea40c48d6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18Px9uQyY1fGGyEAQhzt3h4yDQonU_Sgm\n",
            "To: /content/part2_images.zip\n",
            "\r  0% 0.00/78.4k [00:00<?, ?B/s]\r100% 78.4k/78.4k [00:00<00:00, 69.5MB/s]\n",
            "Archive:  /content/part2_images.zip\n",
            "   creating: /content/part2_images/\n",
            " extracting: /content/part2_images/Bone_1.png  \n",
            " extracting: /content/part2_images/elephant_1.png  \n",
            " extracting: /content/part2_images/brick_2.png  \n",
            " extracting: /content/part2_images/Heart_2.png  \n",
            " extracting: /content/part2_images/Bone_2.png  \n",
            "  inflating: /content/part2_images/elephant_2.png  \n",
            " extracting: /content/part2_images/brick_1.png  \n",
            " extracting: /content/part2_images/Heart_1.png  \n",
            "  inflating: /content/part2_images/device7_1.png  \n",
            "  inflating: /content/part2_images/device7_2.png  \n",
            " extracting: /content/part2_images/fork_2.png  \n",
            " extracting: /content/part2_images/turtle_2.png  \n",
            "  inflating: /content/part2_images/fork_1.png  \n",
            " extracting: /content/part2_images/turtle_1.png  \n",
            " extracting: /content/part2_images/butterfly_2.png  \n",
            " extracting: /content/part2_images/car_1.png  \n",
            " extracting: /content/part2_images/bottle_1.png  \n",
            " extracting: /content/part2_images/bat_2.png  \n",
            " extracting: /content/part2_images/jar_2.png  \n",
            "  inflating: /content/part2_images/hammer_1.png  \n",
            " extracting: /content/part2_images/carriage_2.png  \n",
            " extracting: /content/part2_images/bird_2.png  \n",
            " extracting: /content/part2_images/butterfly_1.png  \n",
            " extracting: /content/part2_images/car_2.png  \n",
            " extracting: /content/part2_images/bottle_2.png  \n",
            " extracting: /content/part2_images/jar_1.png  \n",
            " extracting: /content/part2_images/bat_1.png  \n",
            " extracting: /content/part2_images/hammer_2.png  \n",
            " extracting: /content/part2_images/carriage_1.png  \n",
            " extracting: /content/part2_images/bird_1.png  \n",
            " extracting: /content/part2_images/chicken_1.png  \n",
            " extracting: /content/part2_images/camel_2.png  \n",
            " extracting: /content/part2_images/horse_1.png  \n",
            " extracting: /content/part2_images/bell_1.png  \n",
            " extracting: /content/part2_images/children_1.png  \n",
            " extracting: /content/part2_images/cattle_2.png  \n",
            " extracting: /content/part2_images/cellular_phone_2.png  \n",
            " extracting: /content/part2_images/chicken_2.png  \n",
            " extracting: /content/part2_images/camel_1.png  \n",
            " extracting: /content/part2_images/horse_2.png  \n",
            " extracting: /content/part2_images/bell_2.png  \n",
            " extracting: /content/part2_images/children_2.png  \n",
            " extracting: /content/part2_images/cattle_1.png  \n",
            " extracting: /content/part2_images/cellular_phone_1.png  \n",
            " extracting: /content/part2_images/apple_1.png  \n",
            " extracting: /content/part2_images/face_1.png  \n",
            " extracting: /content/part2_images/dog_1.png  \n",
            " extracting: /content/part2_images/face_2.png  \n",
            " extracting: /content/part2_images/apple_2.png  \n",
            " extracting: /content/part2_images/dog_2.png  \n"
          ]
        }
      ],
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 18Px9uQyY1fGGyEAQhzt3h4yDQonU_Sgm\n",
        "!unzip \"/content/part2_images.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqgb2YNJfQtV"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUgcCzJRAQJ1"
      },
      "source": [
        "## Code (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywdGq5E55thm"
      },
      "outputs": [],
      "source": [
        "def initialize(im1, mean1, mean2, scale_fact_1_to_2, im2, rows, cols):\n",
        "  '''\n",
        "  Apply translation & scaling to im1 as a way of initialization\n",
        "  '''\n",
        "  # Translation\n",
        "  old_img = copy.copy(im1)\n",
        "  im1 = (mean2-mean1) + im1\n",
        "\n",
        "  # Scaling\n",
        "  new_mean = np.mean(im1, axis=0)\n",
        "  im1 = im1 - new_mean\n",
        "  im1 = im1 * scale_fact_1_to_2\n",
        "  im1 = im1 + new_mean\n",
        "  im1[:, 0] = np.clip(im1[:, 0], 0, rows-1)\n",
        "  im1[:, 1] = np.clip(im1[:, 1], 0, cols-1)\n",
        "  return im1\n",
        "\n",
        "def find_NN(i, im2):\n",
        "  distance = 1e8\n",
        "  for j in im2:\n",
        "    curr_distance = np.linalg.norm(i-j)\n",
        "    if curr_distance < distance:\n",
        "      distance = curr_distance\n",
        "      nn = j\n",
        "  return nn\n",
        "\n",
        "def warp_image(im1, m1, m2, m3, m4, t1, t2, rows, cols):\n",
        "  warped_im1 = []\n",
        "  for i in im1:\n",
        "    warped_im1.append([np.clip(m1*i[0]+m2*i[1]+t1, 0, rows-1), np.clip(m3*i[0]+m4*i[1]+t2, 0, cols-1)])\n",
        "  return warped_im1\n",
        "\n",
        "def align_shape(im1, im2, num_iter=50, show_every_iter=False):\n",
        "  '''\n",
        "  im1: input edge image 1\n",
        "  im2: input edge image 2\n",
        "\n",
        "  Output: transformation T [3] x [3]\n",
        "  '''\n",
        "  orig_im2 = copy.copy(im2)\n",
        "  (rows, cols) = im1.shape\n",
        "  idx_grid = np.mgrid[0:rows, 0:cols]\n",
        "  idx_grid = np.swapaxes(idx_grid, 0, 1)\n",
        "  idx_grid = np.swapaxes(idx_grid, 1, 2)\n",
        "  im1 = idx_grid[im1==255] # Gives coordinates of pixels in (row, col) format\n",
        "  im2 = idx_grid[im2==255]\n",
        "  mean1 = np.mean(im1, axis=0)\n",
        "  mean2 = np.mean(im2, axis=0)\n",
        "  scale1 = np.std(im1, axis=0)\n",
        "  scale2 = np.std(im2, axis=0)\n",
        "  scale_fact_1_to_2 = scale2 / scale1\n",
        "  im1 = initialize(im1, mean1, mean2, scale_fact_1_to_2, im2, rows, cols)\n",
        "\n",
        "  for iter in range(num_iter):\n",
        "    matches = []\n",
        "    for i in im1:\n",
        "      nn_of_i = find_NN(i, im2)\n",
        "      matches.append([i, nn_of_i])\n",
        "    A = []\n",
        "    B = []\n",
        "    for match in matches:\n",
        "      x = match[0][0]\n",
        "      y = match[0][1]\n",
        "      x_dash = match[1][0]\n",
        "      y_dash = match[1][1]\n",
        "      A.append([x, y, 0, 0, 1, 0])\n",
        "      A.append([0, 0, x, y, 0, 1])\n",
        "      B.append(x_dash)\n",
        "      B.append(y_dash)\n",
        "    m1, m2, m3, m4, t1, t2 = np.linalg.lstsq(A, B, rcond=None)[0]\n",
        "    im1 = warp_image(im1, m1, m2, m3, m4, t1, t2, rows, cols)\n",
        "    if (show_every_iter):\n",
        "      test = copy.copy(orig_im2)\n",
        "      for i in range(len(im1)):\n",
        "        test[int(im1[i][0]), int(im1[i][1])] = 255\n",
        "      cv2_imshow(test)\n",
        "  return im1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itHJzXsKfSpQ"
      },
      "outputs": [],
      "source": [
        "def evalAlignment(aligned1, im2):\n",
        "  '''\n",
        "  Computes the error of the aligned image (aligned1) and im2, as the\n",
        "  average of the average minimum distance of a point in aligned1 to a point in im2\n",
        "  and the average minimum distance of a point in im2 to aligned1.\n",
        "  '''\n",
        "  d2 = ndimage.distance_transform_edt(1-im2) #distance transform\n",
        "  err1 = np.mean(np.mean(d2[aligned1 > 0]))\n",
        "  d1 = ndimage.distance_transform_edt(1-aligned1);\n",
        "  err2 = np.mean(np.mean(d2[im2 > 0]))\n",
        "  err = (err1+err2)/2;\n",
        "  return err\n",
        "\n",
        "def displayAlignment(im1, im2, aligned1, thick=False):\n",
        "  '''\n",
        "  Displays the alignment of im1 to im2\n",
        "     im1: first input image to alignment algorithm (im1(y, x)=1 if (y, x) \n",
        "      is an original point in the first image)\n",
        "     im2: second input image to alignment algorithm\n",
        "     aligned1: new1(y, x) = 1 iff (y, x) is a rounded transformed point from the first time \n",
        "     thick: true if a line should be thickened for display\n",
        "  ''' \n",
        "  if thick:\n",
        "    # for thick lines (looks better for final display)\n",
        "    dispim = np.concatenate((cv2.dilate(im1.astype('uint8'), np.ones((3,3), np.uint8), iterations=1), \\\n",
        "                             cv2.dilate(aligned1.astype('uint8'), np.ones((3,3), np.uint8), iterations=1), \\\n",
        "                             cv2.dilate(im2.astype('uint8'), np.ones((3,3), np.uint8), iterations=1)), axis=-1)\n",
        "  else:\n",
        "    # for thin lines (faster)\n",
        "    dispim = np.concatenate((im1, aligned1, im2), axis = -1)\n",
        "  return dispim\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vCuTHeShm41"
      },
      "outputs": [],
      "source": [
        "imgPath = '/content/part2_images/';\n",
        "\n",
        "objList = ['apple', 'bat', 'bell', 'bird', 'Bone', 'bottle', 'brick', \\\n",
        "    'butterfly', 'camel', 'car', 'carriage', 'cattle', 'cellular_phone', \\\n",
        "    'chicken', 'children', 'device7', 'dog', 'elephant', 'face', 'fork', 'hammer', \\\n",
        "    'Heart', 'horse', 'jar', 'turtle']\n",
        "\n",
        "numObj = len(objList)\n",
        "\n",
        "for idx in range(11, numObj):\n",
        "  start_time = time.time()\n",
        "  im1 = cv2.imread(imgPath+objList[idx]+'_1.png', 0)\n",
        "  im2 = cv2.imread(imgPath+objList[idx]+'_2.png', 0)\n",
        "  (rows, cols) = im1.shape\n",
        "\n",
        "  aligned_im1_coords = align_shape(im1, im2)\n",
        "  end_time = time.time()\n",
        "  aligned_im1_coords = np.array(aligned_im1_coords)\n",
        "  aligned_im1 = np.zeros((rows, cols))\n",
        "  for i in range(np.shape(aligned_im1_coords)[0]):\n",
        "    aligned_im1[int(aligned_im1_coords[i][0]), int(aligned_im1_coords[i][1])] = 255\n",
        "  print('Runtime for ', objList[idx], '= ', end_time - start_time, ' seconds')\n",
        "  error = evalAlignment(aligned_im1, im2)\n",
        "  dispim = displayAlignment(im1, im2, aligned_im1, thick=True)\n",
        "  print('Error for ', objList[idx], '= ', error)\n",
        "  cv2_imshow(dispim)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AmYt5Vu9BGq"
      },
      "source": [
        "## Write-up (15 pt)\n",
        "\n",
        "1. (5 pts) Give a brief explanation of your algorithm, initialization, and model of the transformation.\n",
        "\n",
        "2. (10 pts) For each result, give:\n",
        "  1.   The alignment display\n",
        "  2.   The final error\n",
        "  3.   The runtime\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRUDQ9Xk5e3v"
      },
      "source": [
        "#### 1) Algorithm, initialization & Transformation model:\n",
        "\n",
        "##### - Algorithm:\n",
        "- Firstly, the edge points in both the images are calculated (by storing the coordinates of the pixels whose value is 255).\n",
        "- Secondly, these arrays of edge points are used to generate an initial transformation as described in the next sub-section.\n",
        "- Then, the Iterative Closest Point algorithm is applied in the following way:\n",
        "  - Determine the match for a point in image1 by finding the point in image2 which has the smallest Euclidean distance. Repeat this for all the points in image1.\n",
        "  - Get the transformation that satisfies these pairs of matches as close as possible as described in the 'Transformation model' sub-section below.\n",
        "  - Using the estimated transformation matrix, warp all the points in image1.\n",
        "  - Repeat the above steps feeding the output of one iteration as the input for the next iteration.\n",
        "- The output of the final iteration gives the final version of image1 aligned with image2.\n",
        "\n",
        "##### - Initialization:\n",
        "  - The first image is translated & scaled in the following way to make it look closer to the second image.\n",
        "  - First, the difference in the centroids between both the images is calculated & all the points in image1 are translated by this value. Let's call this image `translated_img`.\n",
        "  - Then, the standard deviation of each image is calculated & the scale factor is defined as `std_dev of image2 / std_dev of image1`.\n",
        "  - Finally, scaling is performed by radially stretching (from centroid) every point in `translated_img` by this `scale factor`.\n",
        "  - <ins>Note</ins>: 'image' in this sub-section referes to the array of edge points in a picture.\n",
        "\n",
        "##### - Transformation model:\n",
        "  - An affine transformation model is used to fit the pairs of matches between both the images.\n",
        "  - This is done using Direct Linear transformation.\n",
        "  - A Least squares approach is used to find the parametres of the transformation.\n",
        "  - <ins>Note</ins>: 'image' in this sub-section refers to the array of edge points in a picture.\n",
        "\n",
        "#### 2) Results:\n",
        "- <img src=\"https://drive.google.com/uc?id=19IrJ3AsmIOTIHt-fVArm0QxqNOF6hlhc\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:194\n",
        "  - Error:174\n",
        "- <img src=\"https://drive.google.com/uc?id=106IkD29-eYkdbj-hKnsCFebqGlgKjwu-\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:1183\n",
        "  - Error:425\n",
        "- <img src=\"https://drive.google.com/uc?id=1M1tDu-_0yUv4-3E_GRUd8B44m4Nk8X8x\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:125\n",
        "  - Error:179\n",
        "- <img src=\"https://drive.google.com/uc?id=1HAGqeu1c-7OtRKgBff5ALAGFNLPmnwvi\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:469\n",
        "  - Error:221\n",
        "- <img src=\"https://drive.google.com/uc?id=1z9K5JhKtOJtCbEugL9OsHSfwaKfJrLCz\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:528\n",
        "  - Error:320\n",
        "- <img src=\"https://drive.google.com/uc?id=1Q-hvSrUirBOOO-2rlIG8eyIs2rCo8b3c\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:49\n",
        "  - Error:246\n",
        "- <img src=\"https://drive.google.com/uc?id=1BzhlGskNeLPRr-fS6u9PtwheGVIKjMDd\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:115\n",
        "  - Error:303\n",
        "- <img src=\"https://drive.google.com/uc?id=1Bu-wYZDRw_-Vg4fdV3wT1kKVZ_lGaBXc\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:464\n",
        "  - Error:212\n",
        "- <img src=\"https://drive.google.com/uc?id=1aDSvtY_TfkCEgBJ1KqCMYyCR-adaUX8Y\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:868\n",
        "  - Error:269\n",
        "- <img src=\"https://drive.google.com/uc?id=1KPNNB3MMbnp9dn4ETaHhXgPsb7ewfcAp\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:91\n",
        "  - Error:242\n",
        "- <img src=\"https://drive.google.com/uc?id=1U0eas9COqcj2R7Zragppsb-BpSLdOrdz\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:211\n",
        "  - Error:284\n",
        "- <img src=\"https://drive.google.com/uc?id=1YzWMeo0QmWVkgCWzwWMU-L7pv06cFEm2\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:2742\n",
        "  - Error:392\n",
        "- <img src=\"https://drive.google.com/uc?id=1c8gtSRW2c6HF3D7-oiw5MZknJ18-kKDv\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:360\n",
        "  - Error:258\n",
        "- <img src=\"https://drive.google.com/uc?id=1FbAgczb6bzJ-nuzvC1xIRWBSNrYnjHZO\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:237\n",
        "  - Error:144\n",
        "- <img src=\"https://drive.google.com/uc?id=1T4w99rLLFI-e37_lseIOga-9zWuuzEiX\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:63\n",
        "  - Error:155\n",
        "- <img src=\"https://drive.google.com/uc?id=1aUcR-J3ViBLqDU2QBjBI7_rwqex4FHzh\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:5258\n",
        "  - Error:382\n",
        "- <img src=\"https://drive.google.com/uc?id=1Jyw2tEcqXja4YiWf6-7qV0vWRS9xp2-f\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:2141\n",
        "  - Error:351\n",
        "- <img src=\"https://drive.google.com/uc?id=1V3GtwterQUnx1wcCqVizxdCObSQgsPrQ\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:4196\n",
        "  - Error:562\n",
        "- <img src=\"https://drive.google.com/uc?id=1gpmsr3IOw91QtGz4u-ueMLH_YXwXUutm\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:222\n",
        "  - Error:232\n",
        "- <img src=\"https://drive.google.com/uc?id=1wHkWNe9WuflBibGXfP-PrgZyCS9NhUY_\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:1409\n",
        "  - Error:357\n",
        "- <img src=\"https://drive.google.com/uc?id=1V4lUww35Z6kerzIcy2JsJJLb0_JZ8Cbp\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:151\n",
        "  - Error:156\n",
        "- <img src=\"https://drive.google.com/uc?id=1l6YYA8EeLd2pPNLFM_Gfgi6qs5rW0rQp\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:677\n",
        "  - Error:311\n",
        "- <img src=\"https://drive.google.com/uc?id=1bYlV3GaT-yhm_kB9gMM0vPKaS30mHlvb\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:4387\n",
        "  - Error:492\n",
        "- <img src=\"https://drive.google.com/uc?id=1pXpbBho3Oq9pKGPypzebZVXechqKUNTa\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:1463\n",
        "  - Error:386\n",
        "- <img src=\"https://drive.google.com/uc?id=1hSmUT9s_iOAknjDx7peMoZK4vl4iqwQE\" align=\"center\"/>\n",
        "\n",
        "  - Runtime:400\n",
        "  - Error:250\n",
        "\n",
        "<ins>Note</ins>: The above runtimes are in seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6UrONbD4LcH"
      },
      "source": [
        "# Part 3: Object Instance Recognition (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGoM9Ra_4QLS"
      },
      "source": [
        "## Overview\n",
        "This problem explores the Lowe-style object instance recognition.\n",
        "\n",
        "Implement the nearest neighbor distance ratio test using the pre-computed SIFT features SIFT_features.mat provided in the supplementary material. The Frame1, Frame2 indicate the 2D position, scales, and the orientation of the descriptors and Descriptor1, Descriptor2 are the correspondin 128-D SIFT features. Display the matches like this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1mULTvHYeP5uj_vi7nwWThBHkDbv1eSue\" width=\"1000\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDy-8Y7Oj7kJ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0AUhSYpj8ed"
      },
      "outputs": [],
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 10ByzpFbB-z178VGjwmCwc95wInD8vpNM # SIFT Features\n",
        "!gdown 1KLWGMtDEMNNrmzd3Qezrs2-NQR52OfoU # Stop sign image 1\n",
        "!gdown 13y-o1vdGN6CqqPuUcgU7pIxODTxrYS7J # Stop sign image 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQW8iyjH_Ijd"
      },
      "source": [
        "## Code (10 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I3eiXvz_IGj"
      },
      "outputs": [],
      "source": [
        "def compare(x):\n",
        "  return x[0]\n",
        "\n",
        "img1 = cv2.imread('/content/stop1.jpg')\n",
        "img2 = cv2.imread('/content/stop2.jpg')\n",
        "\n",
        "## inside the sift are:\n",
        "## Descriptor1, Descriptor2: SIFT features from image 1 and image 2\n",
        "## Frame1, Frame2: position, scale, rotation of keypoints\n",
        "data = loadmat('/content/SIFT_features.mat')\n",
        "Frame1 = data['Frame1']\n",
        "Descriptor1 = data['Descriptor1']\n",
        "Frame2 = data['Frame2']\n",
        "Descriptor2 = data['Descriptor2']\n",
        "\n",
        "Frame1 = np.transpose(Frame1)\n",
        "Frame2 = np.transpose(Frame2)\n",
        "Descriptor1 = np.transpose(Descriptor1).astype('float32')\n",
        "Descriptor2 = np.transpose(Descriptor2).astype('float32')\n",
        "\n",
        "\n",
        "matches_NN = []\n",
        "matches_2NN = []\n",
        "threshold_NN = 120\n",
        "threshold_2NN = 0.548\n",
        "data_type = [['distance', 'float32'], ['idx', int]]\n",
        "for i in range(Descriptor1.shape[0]):\n",
        "  diff = Descriptor2 - Descriptor1[i]\n",
        "  dist = np.expand_dims(np.linalg.norm(diff, axis=1), axis=1)\n",
        "  temp = np.expand_dims([t for t in range(dist.shape[0])], axis=1)\n",
        "  dist = np.concatenate((dist, temp), axis=1).tolist()\n",
        "  dist.sort(key=compare)\n",
        "\n",
        "  if dist[0][0] < threshold_NN:\n",
        "    matches_NN.append([i, int(dist[0][1])])\n",
        "  if (dist[0][0]/dist[1][0]) < threshold_2NN:\n",
        "    matches_2NN.append([i, int(dist[0][1])])\n",
        "\n",
        "## Display the matched keypoints\n",
        "(rows1, cols1, _) = img1.shape\n",
        "(rows2, cols2, _) = img2.shape\n",
        "combined_img = np.zeros((max(rows1, rows2), cols1+cols2, 3))\n",
        "combined_img[:rows1, :cols1, :] = img1\n",
        "combined_img[:, cols1:cols1+cols2, :] = img2\n",
        "combined_img2 = copy.copy(combined_img)\n",
        "\n",
        "print(len(matches_NN), len(matches_2NN))\n",
        "for match_NN in matches_NN:\n",
        "  combined_img = cv2.line(combined_img, (int(Frame1[match_NN[0]][0]), int(Frame1[match_NN[0]][1])), (int(Frame2[match_NN[1]][0])+cols1, int(Frame2[match_NN[1]][1])), (0, 255, 0), 2)\n",
        "\n",
        "for match_NN in matches_2NN:\n",
        "  combined_img2 = cv2.line(combined_img2, (int(Frame1[match_NN[0]][0]), int(Frame1[match_NN[0]][1])), (int(Frame2[match_NN[1]][0])+cols1, int(Frame2[match_NN[1]][1])), (0, 255, 0), 2)\n",
        "\n",
        "cv2_imshow(combined_img)\n",
        "cv2_imshow(combined_img2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ApiN9gAP6Z"
      },
      "source": [
        "## Write-up (10 pts)\n",
        "\n",
        "(5 pts) Display:\n",
        "\n",
        "1. the matches by thresholding nearest neighbor distances.\n",
        "\n",
        "2. the matches by thresholding the distance ratio. \n",
        "\n",
        "(5 pts) Describe the differences of (1) and (2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEa_-lFkux7C"
      },
      "source": [
        "#### Display:\n",
        " - Matches by thresholding nearest neighbor distances:\n",
        " - <img src=\"https://drive.google.com/uc?id=1LxXW19UoMUcZb0zmVWYnU1rJh5tREOGh\" align=\"center\"/>\n",
        " \n",
        " - Matches by thresholding distance ratio:\n",
        " - <img src=\"https://drive.google.com/uc?id=1EJzAiMPLDfL5VSTBSlUtRp0bEGwdumvo\" align=\"center\"/>\n",
        "\n",
        "#### Note:\n",
        "  - The distance ratio method is better at telling apart close-looking but dissimilar features.\n",
        "  - It has a better False-Positive rate.\n",
        "  - It can be observed (as shown below) that the nearest neighbor distance method falsely matched 2 SIFT descriptors on the white border of the stop sign. But the distance ratio method didn't do such a mistake.\n",
        "  - <img src=\"https://drive.google.com/uc?id=10vRqvAc6b3J651YPGjcsmMzLKciIoufH\" align=\"center\"/>\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qWCfxrUA7LNb",
        "ZDy-8Y7Oj7kJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
