{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKARgqUEG9Nd"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Cee3vAcLrOs0"
   },
   "outputs": [],
   "source": [
    "## Import necessary libraries here (You can add libraries you want to use here)\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from google.colab.patches import cv2_imshow\n",
    "import itertools\n",
    "import copy\n",
    "from math import sqrt\n",
    "import math\n",
    "from scipy.linalg import sqrtm\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3j4FysdrPpj"
   },
   "source": [
    "# Part 1: Epipolar Geometry (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PQh0Alx6ibx"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this problem, you will implement an algorithm for automatically estimating homography with RANSAC. In the file matches.mat, we provide the detected Harris corners row-column positions in variables r1 c1 for the first image; variables r2 c2 for the second image; and the corresponding matched pairs in the variable matches.\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1Tr723u5OXmwkd4RDmu9z886ITJU9j1cL&export=download\" width=\"800\"/> -->\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=17mwO8QH24vw1Kv1aBONgFXKi53HqUMEd&export=download\" width=\"800\"/>\n",
    "\n",
    "\n",
    "The outline of the normalized 8-point algorithm:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1nVnvBpKeLmiowT9Q4_QauogXpcdXBmHm&export=download\" width=\"700\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhMdiMNjB9R7"
   },
   "source": [
    "### Data\n",
    "\n",
    "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "3LNhf2S4CAIk",
    "outputId": "233360b4-9881-4b4e-ac81-f3ffab0c0f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1cn3_SscjlLrf4BzUWe8MV-XqMqBY4Nj_\n",
      "To: /content/Part1_data.zip\n",
      "\r\n",
      "  0% 0.00/157k [00:00<?, ?B/s]\r\n",
      "100% 157k/157k [00:00<00:00, 77.2MB/s]\n",
      "Archive:  /content/Part1_data.zip\n",
      "   creating: /content/Part1_data/\n",
      "  inflating: /content/Part1_data/chapel00.png  \n",
      "  inflating: /content/Part1_data/chapel01.png  \n",
      "  inflating: /content/Part1_data/matches.mat  \n"
     ]
    }
   ],
   "source": [
    "# Download Data -- run this cell only one time per runtime\n",
    "!gdown 1cn3_SscjlLrf4BzUWe8MV-XqMqBY4Nj_\n",
    "!unzip \"/content/Part1_data.zip\" -d \"/content/\"\n",
    "# Load Matches\n",
    "data = loadmat('/content/Part1_data/matches.mat')\n",
    "r1 = data['r1']\n",
    "r2 = data['r2']\n",
    "c1 = data['c1']\n",
    "c2 = data['c2']\n",
    "matches = data['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7JTA_rZ4y_V8"
   },
   "outputs": [],
   "source": [
    "# Load Keypoints\n",
    "x1 = c1[matches[:,0]-1]\n",
    "y1 = r1[matches[:,0]-1]\n",
    "x2 = c2[matches[:,1]-1]\n",
    "y2 = r2[matches[:,1]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjIGNyapNU_4"
   },
   "source": [
    "### Code (15 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS5zihq7NRjh"
   },
   "outputs": [],
   "source": [
    "def normalize(x, y):\n",
    "  # Function: find the transformation to make it zero mean and the variance as sqrt(2)\n",
    "  std_x = np.std(x)\n",
    "  std_y = np.std(y)\n",
    "  mean_x = np.mean(x)\n",
    "  mean_y = np.mean(y)\n",
    "  T = np.array([[1/std_x,     0  , -mean_x/std_x],\n",
    "                [    0  , 1/std_y, -mean_y/std_y],\n",
    "                [    0  ,     0  ,      1       ]])\n",
    "  normalized_coords = []\n",
    "  for i in range(x.shape[0]):\n",
    "    temp = np.matmul(T, np.array([x[i], y[i], 1], dtype=object))\n",
    "    normalized_coords.append([temp[0][0], temp[1][0], temp[2][0]])\n",
    "  normalized_coords = np.array(normalized_coords)\n",
    "  normalized_x = normalized_coords[:, 0]\n",
    "  normalized_y = normalized_coords[:, 1]\n",
    "  return normalized_x, normalized_y, T\n",
    "  \n",
    "def ransacF(x1, y1, x2, y2, do_normalization=True, T1=None, T2=None):\n",
    "  if do_normalization:\n",
    "    # Find normalization matrix & Transform point set 1 and 2\n",
    "    x1, y1, T1 = normalize(x1, y1)\n",
    "    x2, y2, T2 = normalize(x2, y2)\n",
    "  # RANSAC based 8-point algorithm\n",
    "  threshold = 0.1\n",
    "  max_num_inliers = -1\n",
    "  num_iters = 0\n",
    "  rand_indices = [i for i in range(x1.shape[0])]\n",
    "  random.shuffle(rand_indices)\n",
    "  for subset in itertools.combinations(rand_indices, 8):\n",
    "    num_iters += 1\n",
    "    if num_iters>1400:\n",
    "      break\n",
    "    x1_subset = x1[tuple([subset])]\n",
    "    y1_subset = y1[tuple([subset])]\n",
    "    x2_subset = x2[tuple([subset])]\n",
    "    y2_subset = y2[tuple([subset])]\n",
    "    F = computeF(x1_subset,y1_subset,x2_subset,y2_subset)\n",
    "    num_inliers, distances = getInliers(x1, y1, x2, y2, F, threshold)\n",
    "    if num_inliers > max_num_inliers:\n",
    "      inlier_indices = tuple(np.arange(x1.shape[0])[distances==-1])\n",
    "      max_num_inliers = num_inliers\n",
    "      max_inlier_comb = inlier_indices \n",
    "  \n",
    "  print('Number of iterations =', num_iters)\n",
    "  print('Number of inliers =', max_num_inliers)\n",
    "  x1_subset = x1[tuple([max_inlier_comb])]\n",
    "  y1_subset = y1[tuple([max_inlier_comb])]\n",
    "  x2_subset = x2[tuple([max_inlier_comb])]\n",
    "  y2_subset = y2[tuple([max_inlier_comb])]\n",
    "  F_norm = computeF(x1_subset,y1_subset,x2_subset,y2_subset)\n",
    "  F = np.matmul(T1.T, np.matmul(F_norm, T2))\n",
    "  return F, max_inlier_comb\n",
    "\n",
    "def getInliers(x1, y1, x2, y2, F, threshold):\n",
    "  # Function: implement the criteria checking inliers.\n",
    "  distances = []\n",
    "  for j in range(x1.shape[0]):\n",
    "    line = np.matmul(F, np.array([x2[j], y2[j], 1]))\n",
    "    temp = abs(np.matmul(np.array([x1[j], y1[j], 1]), line)) / sqrt(line[0]**2 + line[1]**2)\n",
    "    distances.append(temp)\n",
    "  distances = np.array(distances)\n",
    "  assert np.amin(distances) >= 0, str(np.amin(distances))+' is the min number'\n",
    "  distances[distances<=threshold] = -1\n",
    "  distances[distances>threshold] = 0\n",
    "  num_inliers = np.sum(distances) / -1\n",
    "  return num_inliers, distances\n",
    "\n",
    "  \n",
    "def computeF(x1, y1, x2, y2):\n",
    "  #  Function: compute fundamental matrix from corresponding points\n",
    "  A = []\n",
    "  for i in range(x1.shape[0]):\n",
    "    A.append([x1[i]*x2[i], x1[i]*y2[i], x1[i], y1[i]*x2[i], y1[i]*y2[i], y1[i], x2[i], y2[i], 1])\n",
    "  U, S, VT = np.linalg.svd(A)\n",
    "  f = VT[-1]\n",
    "  F = np.reshape(f, (3,3))\n",
    "  u, s, vt = np.linalg.svd(F)\n",
    "  s[-1] = 0\n",
    "  s = np.diag(s)\n",
    "  F = np.matmul(u, np.matmul(s, vt))\n",
    "  return F\n",
    "\n",
    "def gen_epi_line_and_plot(epi_line, x, y, img):\n",
    "  rows, cols, _ = img.shape\n",
    "  start1 = (0, int(-epi_line[2][0]/epi_line[1][0]))\n",
    "  x_ = cols-1\n",
    "  y_ = int(((-epi_line[0][0]*x_)-epi_line[2][0]) / epi_line[1][0])\n",
    "  end1 = (x_, y_)\n",
    "  img = cv2.line(img, start1, end1, (0, 255, 0), 1)\n",
    "  img = cv2.circle(img, (int(x), int(y)), 2, (0, 0, 255), -1)\n",
    "  return img\n",
    "\n",
    "def plot_epi_lines(epi_line_1, epi_line_2, x_1, y_1, x_2, y_2, im1, im2, show_result):\n",
    "  im1 = gen_epi_line_and_plot(epi_line_1, x_1, y_1, im1)\n",
    "  im2 = gen_epi_line_and_plot(epi_line_2, x_2, y_2, im2)\n",
    "  if show_result:\n",
    "    combined_img = np.hstack((im1, im2))\n",
    "    cv2_imshow(combined_img)\n",
    "\n",
    "img1 = cv2.imread('/content/Part1_data/chapel00.png')\n",
    "img2 = cv2.imread('/content/Part1_data/chapel01.png')\n",
    "F, max_inlier_comb = ransacF(x1, y1, x2, y2)\n",
    "F_unit = F / np.sqrt(np.sum(np.square(F)))\n",
    "print(F_unit)\n",
    "rand_7_indices=[]\n",
    "while np.unique(rand_7_indices).shape[0] != 7:\n",
    "  rand_7_indices = (np.random.rand(7)*x1.shape[0]).astype(int)\n",
    "\n",
    "for rand_idx in rand_7_indices:\n",
    "  x_1 = x1[rand_idx]\n",
    "  y_1 = y1[rand_idx]\n",
    "  x_2 = x2[rand_idx]\n",
    "  y_2 = y2[rand_idx]\n",
    "  epi_line_1 = np.matmul(F, np.array([x_2, y_2, 1], dtype=object))\n",
    "  epi_line_2 = np.matmul(F.T, np.array([x_1, y_1, 1], dtype=object))\n",
    "  plot_epi_lines(epi_line_1, epi_line_2, x_1, y_1, x_2, y_2, img1, img2, rand_idx==rand_7_indices[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUvFYC17Bi5q"
   },
   "source": [
    "### Write-up (15 pt)\n",
    "*   Describe what test you used for deciding inlier vs. outlier.\n",
    "*   Display the estimated fundamental matrix F after normalizing to unit length\n",
    "*   Randomly select 7 sets of matching points. Plot the corresponding epipolar lines and the points on each image. Show the two images (with plotted points and lines) next to each other.\n",
    "\n",
    "<!-- *   Plot the outlier keypoints with green dots on top of the first image -->\n",
    "<!-- *   Randomly select 7 sets of matching points. Plot the corresponding epipolar lines ('g’) and the points (with 'r+’) on each image. Show the two images (with plotted points and lines) next to each other. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JB8fYJCBI6B"
   },
   "source": [
    "- 1) Given a pair of matches x1 & x2, I computed the epipolar line of x2 in image1 & found the distance of the point x1 from the above computed epipolar line in img1 and thresholded it to decide if the pair of matches is an outlier.\n",
    "\n",
    "- 2) Fundamental Matrix:\n",
    "\n",
    "  <img src=\"https://drive.google.com/uc?id=1OdJPrN4R5N9Q_Kdk-OBtv_cAnbt_EkKb\" width=\"400\"/>\n",
    "\n",
    "- 3) Epipolar Lines & associated points:\n",
    "\n",
    "  <img src=\"https://drive.google.com/uc?id=1BLWXQE0PlyWIvBNsqK_qhNRYkK9h7YSq\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE_F4Hf5jUif"
   },
   "source": [
    "# Part 2: Image stitching (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSpCKlC_jcde"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1uOI8rpqb_FsR9Fi8GrGPZvICOcgflBj9&export=download\" width=\"800\"/>\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this problem, you will implement an algorithm for automatically estimating the fundamental matrix F using RANSAC and the normalized 8-point algorithm. \n",
    "\n",
    "Image Stitching Algorithm Overview\n",
    "1. Detect keypoints\n",
    "2. Match keypoints\n",
    "3. Estimate homography with matched keypoints (using RANSAC)\n",
    "4. Combine images\n",
    "\n",
    "**Note:**  Do not use existing image stitching code, such as found on the web, and OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EOoxrZurmYz"
   },
   "source": [
    "## Data\n",
    "\n",
    "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Tspp6CyMroUC",
    "outputId": "969c4cab-5217-4d3d-ccb1-a053bd0c43d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fnD0hJ8-_Rngsc-m96ghKtdZAMf0VTjy\n",
      "To: /content/hill.zip\n",
      "\r\n",
      "  0% 0.00/205k [00:00<?, ?B/s]\r\n",
      "100% 205k/205k [00:00<00:00, 90.1MB/s]\n",
      "Archive:  /content/hill.zip\n",
      "  inflating: /content/hill/1.JPG     \n",
      "  inflating: /content/hill/2.JPG     \n",
      "  inflating: /content/hill/3.JPG     \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1v2BFVMV0McuD5BstLvDmo1U9MrFAByS5\n",
      "To: /content/tv.zip\n",
      "100% 130k/130k [00:00<00:00, 71.9MB/s]\n",
      "Archive:  /content/tv.zip\n",
      "  inflating: /content/tv/1.jpg       \n",
      "  inflating: /content/tv/2.jpg       \n",
      "  inflating: /content/tv/3.jpg       \n"
     ]
    }
   ],
   "source": [
    "# Download Data -- run this cell only one time per runtime\n",
    "!gdown 1fnD0hJ8-_Rngsc-m96ghKtdZAMf0VTjy\n",
    "!unzip \"/content/hill.zip\" -d \"/content/hill\"\n",
    "\n",
    "!gdown 1v2BFVMV0McuD5BstLvDmo1U9MrFAByS5\n",
    "!unzip \"/content/tv.zip\" -d \"/content/tv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlVZP1tvMAxH"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "y9gRD27_MCDo"
   },
   "outputs": [],
   "source": [
    "def plot_matches(img1, img2, keypoints_1, keypoints_2, matches):\n",
    "  combined_img = np.hstack((img1, img2))\n",
    "  img1_cols = img1.shape[1]\n",
    "  colors = [(0, 0, 255), (0, 255,0), (255,0,0), (255,255,0), (0,255,255), (255,0,255)]\n",
    "  j=0\n",
    "  for i in range(len(matches)):\n",
    "    pt1 = (round(tuple(keypoints_1[matches[i][0]].pt)[0])          , round(tuple(keypoints_1[matches[i][0]].pt)[1]))\n",
    "    pt2 = (round(tuple(keypoints_2[matches[i][1]].pt)[0])+img1_cols, round(tuple(keypoints_2[matches[i][1]].pt)[1]))\n",
    "    j %= len(colors)\n",
    "    color = colors[j]\n",
    "    j+=1\n",
    "    combined_img = cv2.line(combined_img, pt1, pt2, color, 1)\n",
    "  cv2_imshow(combined_img)\n",
    "\n",
    "def est_homography(src, dest):\n",
    "    N = src.shape[0]\n",
    "    if N != dest.shape[0]:\n",
    "        raise ValueError(\"src and diff should have the same dimension\")\n",
    "    src_h = np.hstack((src, np.ones((N, 1))))\n",
    "    A = np.array([np.block([[src_h[n], np.zeros(3), -dest[n, 0] * src_h[n]],\n",
    "                            [np.zeros(3), src_h[n], -dest[n, 1] * src_h[n]]])\n",
    "                  for n in range(N)])\n",
    "    A = A.reshape(2 * N, 9)\n",
    "    [_, _, V] = np.linalg.svd(A)\n",
    "    return V.T[:, 8].reshape(3, 3)\n",
    "\n",
    "def apply_homography(H, src):\n",
    "    src_h = np.hstack((src, np.ones((src.shape[0], 1))))\n",
    "    dest =  src_h @ H.T\n",
    "    if dest[:,[2]].any() == 0:\n",
    "      print('divide by zero')\n",
    "    return (dest / dest[:,[2]])[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBhvHlseN_6F"
   },
   "source": [
    "### Code (15 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBu2Tu987E7h"
   },
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "  return x[0]\n",
    "\n",
    "def compute_2nn_ratio(Descriptor1, Descriptor2):\n",
    "  diff = Descriptor2 - Descriptor1\n",
    "  dist = np.expand_dims(np.linalg.norm(diff, axis=1), axis=1)\n",
    "  temp = np.expand_dims([t for t in range(dist.shape[0])], axis=1)\n",
    "  dist = np.concatenate((dist, temp), axis=1).tolist()\n",
    "  dist.sort(key=compare)\n",
    "  return dist[0][0]/dist[1][0], int(dist[0][1])\n",
    "\n",
    "def detect_keypoints_and_find_matches(img1, img2, display):\n",
    "  sift = cv2.xfeatures2d.SIFT_create()\n",
    "  keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
    "  keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n",
    "  matches = []\n",
    "  threshold_2NN = 0.45 #0.45\n",
    "  for i in range(len(keypoints_1)):\n",
    "    ratio, closest_match = compute_2nn_ratio(descriptors_1[i], descriptors_2)\n",
    "    if ratio < threshold_2NN:\n",
    "      matches.append([i, closest_match])\n",
    "  print('Number of matches =', len(matches))\n",
    "  if display:\n",
    "    plot_matches(img1, img2, keypoints_1, keypoints_2, matches)\n",
    "  return matches, keypoints_1, keypoints_2\n",
    "\n",
    "def normalize__(img1_shape, img2_shape, matches, keypoints_1, keypoints_2):\n",
    "  x1 = [i for i in range(img1_shape[1])]\n",
    "  y1 = [i for i in range(img1_shape[0])]\n",
    "  x2 = [i for i in range(img2_shape[1])]\n",
    "  y2 = [i for i in range(img2_shape[0])]\n",
    "  (mean_x1, std_x1, mean_y1, std_y1) = [np.mean(x1), np.std(x1), np.mean(y1), np.std(y1)]\n",
    "  (mean_x2, std_x2, mean_y2, std_y2) = [np.mean(x2), np.std(x2), np.mean(y2), np.std(y2)]\n",
    "  T1 = np.array([[1/std_x1,     0   , -mean_x1/std_x1],\n",
    "                [    0    , 1/std_y1, -mean_y1/std_y1],\n",
    "                [    0    ,     0   ,        1       ]])\n",
    "  T2 = np.array([[1/std_x2,     0   , -mean_x2/std_x2],\n",
    "                [    0    , 1/std_y2, -mean_y2/std_y2],\n",
    "                [    0    ,     0   ,        1       ]])\n",
    "  \n",
    "  normalized_coords1 = []\n",
    "  normalized_coords2 = []\n",
    "  for i in range(len(matches)):\n",
    "    (temp1, temp2) = [list(keypoints_1[matches[i][0]].pt), list(keypoints_2[matches[i][1]].pt)]\n",
    "    temp1.append(1)\n",
    "    temp2.append(1)\n",
    "    (temp1, temp2) = [np.array(temp1), np.array(temp2)]\n",
    "    temp1 = T1 @ temp1\n",
    "    temp2 = T2 @ temp2\n",
    "    normalized_coords1.append(temp1)\n",
    "    normalized_coords2.append(temp2)\n",
    "\n",
    "  normalized_coords1 = np.array(normalized_coords1)\n",
    "  normalized_coords2 = np.array(normalized_coords2)\n",
    "  xy1_ = normalized_coords1[:, :-1]\n",
    "  xy2_ = normalized_coords2[:, :-1]\n",
    "  return xy1_, xy2_, T1, T2\n",
    "\n",
    "\n",
    "def stitch_2_images(img1, img2, display):\n",
    "  matches, keypoints_1, keypoints_2 = detect_keypoints_and_find_matches(img1, img2, display)\n",
    "  xy1_, xy2_, T1, T2 = normalize__(img1.shape, img2.shape, matches, keypoints_1, keypoints_2)\n",
    "  distance_threshold = 0.01 #0.01\n",
    "  max_num_inliers = -1\n",
    "  for _ in range(1000):\n",
    "    rand_indices = random.sample(range(0, xy1_.shape[0]), 4)\n",
    "    subset_xy2_ = xy2_[tuple([rand_indices])]\n",
    "    subset_xy1_ = xy1_[tuple([rand_indices])]\n",
    "    H_norm = est_homography(subset_xy1_, subset_xy2_)\n",
    "    xy1_homo = apply_homography(H_norm, xy1_)\n",
    "    diff = xy2_ - xy1_homo\n",
    "    distances = np.linalg.norm(diff, axis=1)\n",
    "    distances[distances<=distance_threshold] = -1\n",
    "    distances[distances>distance_threshold] = 0\n",
    "    num_inliers = np.sum(distances) / -1\n",
    "    if max_num_inliers < num_inliers:\n",
    "      max_num_inliers = num_inliers\n",
    "      max_inlier_comb = tuple(np.arange(xy2_.shape[0])[distances==-1])\n",
    "\n",
    "  print('Max number of inliers =', max_num_inliers)\n",
    "  subset_xy1_ = xy1_[tuple([max_inlier_comb])]\n",
    "  subset_xy2_ = xy2_[tuple([max_inlier_comb])]\n",
    "  H_norm = est_homography(subset_xy1_, subset_xy2_)\n",
    "  H = np.linalg.inv(T2) @ H_norm @ T1\n",
    "  print('De-normalized Homography:\\n', H)\n",
    "  xy1 = np.mgrid[0:img1.shape[1]:0.25, 0:img1.shape[0]:0.25]\n",
    "  xy1 = xy1.reshape((2, -1))\n",
    "  xy1 = np.transpose(xy1)\n",
    "\n",
    "  xy1_homo = apply_homography(H, xy1)\n",
    "  combined_img = np.zeros((img1.shape[0], img1.shape[1]+img2.shape[1], 3)) # define the size of new canvas\n",
    "  combined_img[:, -img2.shape[1]:, :] = img2\n",
    "  counter_rows = 0\n",
    "  counter_cols_L = 0\n",
    "  counter_cols_R = 0\n",
    "  counter_rows_cols = 0\n",
    "  for i in range(xy1_homo.shape[0]):\n",
    "    new_row = int(xy1_homo[i][1])\n",
    "    new_col = int(xy1_homo[i][0]+img1.shape[1])\n",
    "    if (new_row < 0) or (new_row > img1.shape[0]-1) or (new_col > img1.shape[1]+img2.shape[1]-1) or (new_col < 0):\n",
    "      continue\n",
    "    combined_img[new_row, new_col] = img1[int(xy1[i][1]), int(xy1[i][0])]\n",
    "  combined_img = combined_img.astype(np.uint8)\n",
    "  if display:\n",
    "    cv2_imshow(combined_img)\n",
    "  return combined_img\n",
    "\n",
    "\n",
    "# img1 = cv2.imread('/content/tv/1.jpg')\n",
    "# img2 = cv2.imread('/content/tv/2.jpg')\n",
    "# img3 = cv2.imread('/content/tv/3.jpg')\n",
    "img1 = cv2.imread('/content/hill/1.JPG')\n",
    "img2 = cv2.imread('/content/hill/2.JPG')\n",
    "img3 = cv2.imread('/content/hill/3.JPG')\n",
    "stitched_12 = stitch_2_images(img1, img2, True)\n",
    "stitched_123 = stitch_2_images(stitched_12, img3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCSoLSQON2yj"
   },
   "source": [
    "### Write-up (15 pt)\n",
    "*  Describe how to remove incorrect matches with RANSAC \n",
    "*  Display the best homography H after RANSAC \n",
    "*  Display the blended images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGGINt3MCKK6"
   },
   "source": [
    "- 1) RANSAC involves rejecting outliers. It does this by randomly sampling N samples from M (M>N) data points (here, match pairs) & fitting an equation to those N samples. Then, all the M data points are substituted in the above fitted equation & those data points which don't satisfy the equation upto a threshold distance are considered as incorrect matches (outliers) & they are removed.\n",
    "  - This process is repeated for many iterations & the iteration which has the least number of outliers is selected to get the final estimate for the equation.\n",
    "\n",
    "- 2) The following are the de-normalized Homography matrices for stitching (i) the 1st hill image with the 2nd hill image & (ii) the stitched 1st & 2nd hill image with the 3rd hill image:\n",
    "  - <img src=\"https://drive.google.com/uc?id=1z0ipA7VsynFlsna9DXdj_0Q5Lor8tsRu\" width=\"400\"/>\n",
    "  - <img src=\"https://drive.google.com/uc?id=1OAQtWGj9hDfdVMkJfm6a8GpRSPw4b-8V\" width=\"400\"/>\n",
    "\n",
    "- 3) Blended images:\n",
    "  - <img src=\"https://drive.google.com/uc?id=1zVbOac1xOpr8WYGUfwRDfqBL9ozW4KW5\"/>\n",
    "  - <img src=\"https://drive.google.com/uc?id=1qTLNd6yPPMlEbP5xhaOlQz01Tsr7zgGd\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVJf6O25JW2k"
   },
   "source": [
    "# Part 3: Affine Structure from Motion (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGgP6ygcJaqW"
   },
   "source": [
    "## Overview\n",
    "<img src=\"https://drive.google.com/uc?id=1nYd0eJjBtVIPuapfxuiVzswjswGN_Gq2&export=download\" width=\"800\"/>\n",
    "\n",
    "\n",
    "This problem continues the interest point detection and tracking problem from HW2. Now, you will recover a 3D pointcloud from the image sequence hotel.seq0.png … hotel.seq50.png. You are encouraged to use your results from HW2, but in case you were not able to complete it, we have also included pre- computed intermediate results in the supplemental material. Submit your code so that we can reproduce your results.\n",
    "\n",
    "The outline of the affine structure from motion algorithm:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1BSvHwRR5gNBwDGlrk-dcLCRcuIAvab__&export=download\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEdG0AQNK2DN"
   },
   "source": [
    "## Data\n",
    "\n",
    "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "R0c9I8joK4bW",
    "outputId": "6eb958c1-bf38-417b-e191-8f6bbad6f0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1A0Rin_YMmWkExjI99vfLYvU_dy-9gFTT\n",
      "To: /content/Part2_data.zip\n",
      "\r\n",
      "  0% 0.00/5.44M [00:00<?, ?B/s]\r\n",
      "100% 5.44M/5.44M [00:00<00:00, 254MB/s]\n",
      "Archive:  /content/Part2_data.zip\n",
      "   creating: /content/Part2_data/\n",
      "   creating: /content/Part2_data/images/\n",
      "  inflating: /content/Part2_data/images/hotel.seq0.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq1.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq10.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq11.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq12.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq13.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq14.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq15.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq16.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq17.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq18.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq19.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq2.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq20.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq21.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq22.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq23.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq24.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq25.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq26.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq27.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq28.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq29.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq3.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq30.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq31.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq32.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq33.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq34.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq35.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq36.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq37.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq38.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq39.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq4.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq40.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq41.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq42.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq43.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq44.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq45.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq46.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq47.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq48.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq49.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq5.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq50.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq6.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq7.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq8.png  \n",
      "  inflating: /content/Part2_data/images/hotel.seq9.png  \n",
      "  inflating: /content/Part2_data/tracks.mat  \n"
     ]
    }
   ],
   "source": [
    "# Download Data -- run this cell only one time per runtime\n",
    "!gdown 1A0Rin_YMmWkExjI99vfLYvU_dy-9gFTT\n",
    "!unzip \"/content/Part2_data.zip\" -d \"/content/\"\n",
    "# Load Matches\n",
    "data = loadmat('/content/Part2_data/tracks.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzZskEHdNaxT"
   },
   "source": [
    "## Code (20 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo14ELnGNbIr"
   },
   "outputs": [],
   "source": [
    "track_x = data['track_x']\n",
    "track_y = data['track_y']\n",
    "\n",
    "D = np.block([[data['track_x'][:, i], data['track_y'][:, i]] for i in range(data['track_y'].shape[1])]).reshape((102, -1))\n",
    "# Remove the nan values (points which went out of frame)\n",
    "nans = np.isnan(D)\n",
    "keep_indices = []\n",
    "for i in range(D.shape[1]):\n",
    "  d_col = nans[:,i].tolist()\n",
    "  if not any(d_col):\n",
    "    keep_indices.append(i)\n",
    "track_x = track_x[keep_indices, :]\n",
    "track_y = track_y[keep_indices, :]\n",
    "\n",
    "def affineSFM(x, y):\n",
    "  '''\n",
    "  Function: Affine structure from motion algorithm\n",
    "  % Normalize x, y to zero mean\n",
    "  % Create measurement matrix\n",
    "  D = [xn' ; yn'];\n",
    "  % Decompose and enforce rank 3\n",
    "  % Apply orthographic constraints\n",
    "  '''\n",
    "  mean_x = np.mean(x, axis=0)\n",
    "  mean_y = np.mean(y, axis=0)\n",
    "  x -= mean_x\n",
    "  y -= mean_y\n",
    "  D = np.block([[x[:, i], y[:, i]] for i in range(y.shape[1])]).reshape((102, -1))\n",
    "  U, S, VT = np.linalg.svd(D)\n",
    "  U3 = U[:, :3]\n",
    "  S3 = np.diag(S[:3])\n",
    "  VT3 = VT[:3, :]\n",
    "  S3_sqrt = sqrtm(S3)\n",
    "  A_ = U3 @ S3_sqrt\n",
    "  X_ = S3_sqrt @ VT3\n",
    "  scalar_mat = []\n",
    "  for i in range(0, A_.shape[0], 2):\n",
    "    a1 =  A_[i]\n",
    "    a2 = A_[i+1]\n",
    "    block1 = (np.expand_dims(a1, axis=1) @ np.expand_dims(a1, axis=0)).reshape((1,9))\n",
    "    block2 = (np.expand_dims(a2, axis=1) @ np.expand_dims(a2, axis=0)).reshape((1,9))\n",
    "    block3 = (np.expand_dims(a1, axis=1) @ np.expand_dims(a2, axis=0)).reshape((1,9))\n",
    "    if i==0:\n",
    "      coeff_mat = block1\n",
    "    else:\n",
    "      coeff_mat = np.concatenate((coeff_mat, block1), axis=0)\n",
    "    coeff_mat = np.concatenate((coeff_mat, block2), axis=0)\n",
    "    coeff_mat = np.concatenate((coeff_mat, block3), axis=0)\n",
    "    scalar_mat += [1,1,0]\n",
    "  scalar_mat = np.array(scalar_mat)\n",
    "  least_sq = np.linalg.lstsq(coeff_mat, scalar_mat, rcond=None)\n",
    "  L = least_sq[0].reshape((3,3))\n",
    "  w, v = np.linalg.eig(L)\n",
    "  C = np.linalg.cholesky(L)\n",
    "  A = A_ @ C\n",
    "  X = np.linalg.inv(C) @ X_\n",
    "  return A, X\n",
    "\n",
    "def plot_structure(X):\n",
    "  fig = go.Figure(data=[go.Scatter3d(x=X[0, :], y=X[1, :], z=X[2, :], mode='markers',\n",
    "    marker=dict(size=3))])\n",
    "  fig.show()\n",
    "\n",
    "def plot_camera_motion(A):\n",
    "  for i in range(0, A.shape[0], 2):\n",
    "    position = np.cross(A[i], A[i+1])\n",
    "    mag = np.linalg.norm(position)\n",
    "    norm_position = position / mag\n",
    "    norm_position = np.expand_dims(norm_position, axis=0)\n",
    "    if i==0:\n",
    "      cam_motion = norm_position\n",
    "    else:\n",
    "      cam_motion = np.concatenate((cam_motion, norm_position), axis=0)\n",
    "  print(cam_motion.shape)\n",
    "  fig = go.Figure(data=[go.Scatter3d(x=cam_motion[:, 0], y=cam_motion[:, 1], z=cam_motion[:, 2], mode='markers',\n",
    "    marker=dict(size=3))])\n",
    "  # fig = go.Figure()\n",
    "  # fig.add_trace(go.Scatter(x=np.linspace(0, A.shape[0]/2, num=1+A.shape[0]//2), y=cam_motion[:, 0], mode='lines', name='x'))\n",
    "  # fig.add_trace(go.Scatter(x=np.linspace(0, A.shape[0]/2, num=1+A.shape[0]//2), y=cam_motion[:, 1], mode='lines', name='y'))\n",
    "  # fig.add_trace(go.Scatter(x=np.linspace(0, A.shape[0]/2, num=1+A.shape[0]//2), y=cam_motion[:, 2], mode='lines', name='z'))\n",
    "  fig.show()\n",
    "\n",
    "A, X = affineSFM(track_x, track_y)\n",
    "print(A.shape, X.shape)\n",
    "plot_structure(X)\n",
    "plot_camera_motion(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZgGQcmAJhVQ"
   },
   "source": [
    "### Write-up (20 pt)\n",
    "\n",
    "\n",
    "*   Plot the predicted 3D locations of the tracked points for 3 different viewpoints. Choose the viewpoints so that the 3D structure is clearly visible.\n",
    "*   Plot the predicted 3D path of the cameras. The camera position for each frame is given by the cross product a_k = a_i x a_j. Normalize a_k to be unit length for consistent results. Give 3 plots, one for each dimension of a_k \n",
    "<!-- We provide the function plotSfM.m for visualizing the recovered 3D shape and camera positions in each frame. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtJ465J8KYid"
   },
   "source": [
    "### Hint\n",
    "\n",
    "\n",
    "- 3D Structure Visualizations:\n",
    "  - <img src=\"https://drive.google.com/uc?id=1j9_aviLnIdeEPcDrt5CcmdNIi-gBj5KD\"/>\n",
    "  - <img src=\"https://drive.google.com/uc?id=1_3sXKJTJswiq4-jo1E2vkPIEP8gXakA0\"/>\n",
    "  - <img src=\"https://drive.google.com/uc?id=1En4G3LZ1NR61qXeWdZegyK2Hyf4t22zq\"/>\n",
    "  - <img src=\"https://drive.google.com/uc?id=185GUlxabZtI2lKH9OwXO5yBjLk1n73rx\"/>\n",
    "\n",
    "- Camera motion:\n",
    "  - 3D plot:\n",
    "    - <img src=\"https://drive.google.com/uc?id=1EtwMMDKceDsgh3S13ZKYWgEff3wl_HnF\"/>\n",
    "  - Separate 2D plots (x, y & z in order):\n",
    "    - <img src=\"https://drive.google.com/uc?id=1BQ-b7zp9qM3w82oCNYbn3ya00Ox83EUJ\"/>\n",
    "    - <img src=\"https://drive.google.com/uc?id=1aqmDNXmDPBdsbwVTdLrwFaJShvOAEvys\"/>\n",
    "    - <img src=\"https://drive.google.com/uc?id=1oUZw8USY99joCoIB4prjl9CbkvW1YZAk\"/>\n",
    "    \n",
    "<!--     \n",
    "\n",
    "*   Reference: \n",
    "    - Tomasi and Kanade. Shape and Motion from Image Streams under Orthography: a Factorization Method. 1992 -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pKARgqUEG9Nd",
    "f3j4FysdrPpj",
    "WhMdiMNjB9R7",
    "_EOoxrZurmYz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
